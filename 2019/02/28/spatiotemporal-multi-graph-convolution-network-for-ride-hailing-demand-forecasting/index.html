<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand Forecasting - Davidham&#039;s blog</title><meta description="AAAI 2019，滴滴的网约车需求预测，5个点预测1个点。空间依赖建模上：以图的形式表示数据，从空间地理关系、区域功能相似度、区域交通连通性三个角度构造了三个不同的图，提出了多图卷积，分别用 k 阶 ChebNet 对每个图做图卷积，然后将多个图的卷积结果进行聚合(sum, average 等)成一个图；时间依赖建模上：提出了融合背景信息的 Contextual Gated RNN (CGRNN"><meta property="og:type" content="blog"><meta property="og:title" content="Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand Forecasting"><meta property="og:url" content="https://davidham3.github.io/2019/02/28/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/"><meta property="og:site_name" content="Davidham&#039;s blog"><meta property="og:description" content="AAAI 2019，滴滴的网约车需求预测，5个点预测1个点。空间依赖建模上：以图的形式表示数据，从空间地理关系、区域功能相似度、区域交通连通性三个角度构造了三个不同的图，提出了多图卷积，分别用 k 阶 ChebNet 对每个图做图卷积，然后将多个图的卷积结果进行聚合(sum, average 等)成一个图；时间依赖建模上：提出了融合背景信息的 Contextual Gated RNN (CGRNN"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://davidham3.github.io/blog/2019/02/28/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Fig1.JPG"><meta property="og:image" content="https://davidham3.github.io/blog/2019/02/28/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Fig2.JPG"><meta property="og:image" content="https://davidham3.github.io/blog/2019/02/28/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Fig3.JPG"><meta property="og:image" content="https://davidham3.github.io/blog/2019/02/28/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Table1.JPG"><meta property="og:image" content="https://davidham3.github.io/blog/2019/02/28/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Table2.JPG"><meta property="og:image" content="https://davidham3.github.io/blog/2019/02/28/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Table3.JPG"><meta property="og:image" content="https://davidham3.github.io/blog/2019/02/28/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Table4.JPG"><meta property="og:image" content="https://davidham3.github.io/blog/2019/02/28/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Fig5.JPG"><meta property="article:published_time" content="2019-02-28T13:12:58.000Z"><meta property="article:modified_time" content="2022-04-25T14:58:30.851Z"><meta property="article:author" content="Davidham"><meta property="article:tag" content="Graph"><meta property="article:tag" content="deep learning"><meta property="article:tag" content="graph convolutional network"><meta property="article:tag" content="Spatial-temporal"><meta property="article:tag" content="Time Series"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/blog/2019/02/28/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Fig1.JPG"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://davidham3.github.io/2019/02/28/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/"},"headline":"Davidham's blog","image":[],"datePublished":"2019-02-28T13:12:58.000Z","dateModified":"2022-04-25T14:58:30.851Z","author":{"@type":"Person","name":"Davidham"},"description":"AAAI 2019，滴滴的网约车需求预测，5个点预测1个点。空间依赖建模上：以图的形式表示数据，从空间地理关系、区域功能相似度、区域交通连通性三个角度构造了三个不同的图，提出了多图卷积，分别用 k 阶 ChebNet 对每个图做图卷积，然后将多个图的卷积结果进行聚合(sum, average 等)成一个图；时间依赖建模上：提出了融合背景信息的 Contextual Gated RNN (CGRNN"}</script><link rel="canonical" href="https://davidham3.github.io/2019/02/28/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/"><link rel="icon" href="/blog/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/"><img src="/blog/img/logo.svg" alt="Davidham&#039;s blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/blog/">Home</a><a class="navbar-item" href="/blog/archives">Archives</a><a class="navbar-item" href="/blog/categories">Categories</a><a class="navbar-item" href="/blog/tags">Tags</a><a class="navbar-item" href="/blog/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2019-02-28T13:12:58.000Z" title="2019-02-28T13:12:58.000Z">2019-02-28</time><span class="level-item"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></span><span class="level-item">41 minutes read (About 6219 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand Forecasting</h1><div class="content"><p>AAAI 2019，滴滴的网约车需求预测，5个点预测1个点。空间依赖建模上：以图的形式表示数据，从空间地理关系、区域功能相似度、区域交通连通性三个角度构造了三个不同的图，提出了多图卷积，分别用 k 阶 ChebNet 对每个图做图卷积，然后将多个图的卷积结果进行聚合(sum, average 等)成一个图；时间依赖建模上：提出了融合背景信息的 Contextual Gated RNN (CGRNN)，用 ChebNet 对每个结点卷积后，得到他们的邻居表示，即每个结点的背景信息表示，与原结点特征拼接，用一个两层全连接神经网络计算出 T 个权重，将权重乘到历史 T 个时刻的图上，对历史值进行缩放，然后用一个共享的 RNN，针对每个结点形成的长度为 T 的时间序列建模，得到每个结点新的时间表示。最后预测每个点的网约车需求。原文地址：<a href="http://www-scf.usc.edu/~yaguang/papers/aaai19_multi_graph_convolution.pdf">Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand Forecasting</a></p>
<a id="more"></a>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>区域级别的预测是网约车服务的关键任务。精确地对网约车需求预测可以指导车辆调度、提高车辆利用率，减少用户的等待时间，减轻交通拥堵。这个任务的关键在于区域间复杂的时空依赖关系。现存的方法主要关注临近区域的欧式关系建模，但是在距离较远的区域间组成的非欧式关系对精确预测也很关键。我们提出了 <em>spatiotemporal multi-graph convolution network</em> (ST-MGCN)。我们首先将非欧的关系对编码到多个图中，然后使用 multi-graph convolution 对他们建模。为了在时间建模上利用全局的背景信息，我们提出了 <em>contextual gated recurrent neural network</em>，用一个注意背景的门机制对不同的历史观测值重新分配权重。在两个数据集上比当前的 state-of-the-art 强 10%。</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>我们研究的问题是区域级别网约车需求预测，是智能运输系统的重要部分。目标是通过历史观测值，预测一个城市里面各区域未来的需求。任务的挑战是复杂的时空关系。一方面，不同区域有着复杂的依赖关系。举个例子，一个区域的需求通常受其空间上临近的区域所影响，同时与有着相同背景的较远的区域有联系。另一方面，非线性的依赖关系也存在于不同的时间观测值之间。预测一个时刻通常和多个历史的观测值相关，比如一小时前、一天前、甚至一周前。</p>
<center><img src="/blog/2019/02/28/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Fig1.JPG" class title="Figure1"></center>

<p>最近在深度学习的进步使得对基于区域级别的时空关系预测有了很好的结果。使用卷积神经网络和循环神经网络，得到了很多非常好的效果(Shi et al. 2015; Yu et al. 2017; Shi et al. 2017; Zhang, Zheng, and Qi 2017; Zhang et al. 2018a; Ma et al. 2017; Yao et al. 2018b; 2018a)。尽管有了很好的效果，但是我们认为在对时空关系建模上有两点被忽略了。其一，这些方法主要对不同区域的欧式关系建模，但是我们发现非欧关系很重要。图 1 是一个例子，对于区域 1，以及邻居区域 2，可能和很远的区域 3 有相似的功能，也就是他们都靠近学校和医院。此外，区域 1 还可能被区域 4 影响，区域 4 是通过高速公路直接与区域 1 相连的。其二：这些方法中，在使用 RNN 对时间关系建模时，每个区域是独立处理的，或者只基于局部信息。然而，我们认为全局和背景信息也很重要。举个例子，网约车需求的一个全局性的增长/减小通常表明一些可能会影响未来需求的活动发生了。</p>
<p>我们提出了 ST-MGCN 解决这些问题。在 ST-MGCN 中，我们提出了将区域间非欧关系编码进多个图的方法。不同于 Yao et al. 2018b 给每个区域使用图嵌入作为额外的不变特征，我们用图卷积对区域间的关系对直接建模。图卷积在预测的时候可以聚合邻居特征，传统的图嵌入难以做到这一点。此外，在对时间关系建模时，为了聚合全局的背景信息，我们提出了 contextual gated recurrent neural network (CGRNN)。通过一个基于全局信息计算的门机制增强 RNN，对不同时间步的观测值重新赋权重。我们在两个大型的真实数据集上做了测试，ST-MGCN 比 baselines 好了一大截。我们主要的贡献是：</p>
<ul>
<li>识别了网约车需求预测中的非欧关系，将他们编码进多个图。利用多图卷积对这些关系建模。</li>
<li>对时间依赖，提出了 Contextual Gated RNN (CGRNN) 来集成全局背景信息。</li>
<li>在两个大型真实数据集上做了实验，提出的方法比 state-of-the-art 在相对误差上小了 10%.</li>
</ul>
<h1 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h1><h2 id="Spatiotemporal-prediction-in-urban-computing"><a href="#Spatiotemporal-prediction-in-urban-computing" class="headerlink" title="Spatiotemporal prediction in urban computing"></a>Spatiotemporal prediction in urban computing</h2><p>时空预测是数据驱动的城市管理的基础问题。有很多关于这方面的工作，自行车流量预测(Zhang, Zheng, and Qi 2017)，出租车需求(Ke et al. 2017b; Yao et al. 2018b)，到达时间(Li et al. 2018b)，降雨量(Shi et al. 2015; 2017)，对矩形区域的聚合值进行预测，区域关系通过地理距离建模。具体来讲，城市数据的空间结构通过矩阵形式表示，每个元素表示一个矩形区域。在之前的工作中，区域和他们的关系对一般表示成欧式结构，使得卷积神经网络可以有效地利用这个结构来预测。</p>
<p>非欧结构的数据也存在于城市计算。通常，基于站点或点的预测任务，像流量预测(Li et al. 2018c; Yu, Yin, and Zhu 2018; Yao et al. 2018a)，基于点的出租车需求预测(Tong et al. 2017)以及基于站点的自行车流量预测(Chai, Wang, and Yang 2018)是很自然的非欧结构，数据不再是矩阵形式，卷积神经网络也不那么有效了。人工定制的特征工程或图卷积网络是处理非欧结构数据目前最好的方法。不同于之前的工作，ST-MGCN 将区域间的关系对编码进语义图中。尽管 ST-MGCN 是对基于区域的预测设计的，但是区域关系的非规整性使得它实际是对非欧数据进行预测。</p>
<p>在 (Yao et al. 2018b)，作者提出 DMVST-Net，将区域间关系编码进图中来预测出租车需求。DMVST-Net 主要使用图嵌入作为额外特征来预测，没有使用相关区域的需求值（目标值）。在 (Yao et al. 2018a) 的工作中，作者通过注意力机制对周期性的平移问题建模提升了性能。但是，这些方法都没有直接对区域间的非欧关系建模。我们的工作中，ST-MGCN 使用提出的多图卷积从相关区域聚合特征，从不同角度的相关区域的预测值中做预测。</p>
<p>最近在对帕金森的神经图像分析 (Zhang et al. 2018b) 的研究中，图卷积在空间特征提取上很有效。他们使用 GCN 从最相似的区域中学习特征，提出了多视图结构融合了不同的 MRI。然而，上述工作没有考虑时间依赖。ST-GCN 用于基于骨骼的动作识别(Li et al. 2018a; Yan, Xiong, and Lin 2018)。ST-GCN 的变换是一个空间依赖和局部时间循环的组合。然而，我们认为这些模型，在时间依赖建模上，背景信息或全局信息被忽略了。</p>
<h2 id="Graph-convolution-network"><a href="#Graph-convolution-network" class="headerlink" title="Graph convolution network"></a>Graph convolution network</h2><p>图卷积网络定义在图 $\mathcal{G} = (V, \boldsymbol{A})$ 上，$V$ 是顶点集，$\boldsymbol{A} \in \mathbb{R}^{\vert V \vert \times \vert V \vert}$ 是邻接矩阵，元素表示顶点间是否相连。GCN 可以用不同的感受野从不同的非欧结构中提取局部特征(Hammond et al. 2011)。令 $\boldsymbol{L} = \boldsymbol{I} - \boldsymbol{D}^{-1/2} \boldsymbol{A} \boldsymbol{D}^{-1/2}$ 表示图拉普拉斯矩阵，$\boldsymbol{D}$ 是度矩阵，图卷积操作 (Defferrard, Bresson, and Vandergheynst 2016) 定义为：</p>
<script type="math/tex; mode=display">
\boldsymbol{X}_{l+1} = \sigma (\sum^{K-1}_{k=0} \alpha_k \boldsymbol{L}^k \boldsymbol{X}_l),</script><p>$\boldsymbol{X}_l$ 表示第 $l$ 层的特征，$\alpha_k$ 表示可学习的参数，$\boldsymbol{L}^k$ 是图拉普拉斯矩阵的 $k$ 次幂，$\sigma$ 是激活函数。</p>
<h2 id="Channel-wise-attention"><a href="#Channel-wise-attention" class="headerlink" title="Channel-wise attention"></a>Channel-wise attention</h2><p>Channel-wise attention (Hu, Shen, and Sun 2018; Chen et al. 2017) 在 cv 的论文中提出。本质是给每个通道学习一个权重，为了找到最重要的帧，然后基于他们更高的权重。$\boldsymbol{X} \in \mathbb{R}^{W \times H \times C}$ 表示输入，$W$ 和 $H$ 是输入图像的维度，$C$ 表示通道数，channel-wise attention 计算方式如下：</p>
<script type="math/tex; mode=display">\tag{1}
z_c = F_{pool}(\boldsymbol{X}_{:,:,c}) = \frac{1}{WH} \sum^W_{i=0} \sum^H_{j=0} X_{i,j,c} \quad \text{for} c=1,2,\dots,C \\
\boldsymbol{s} = \sigma(\boldsymbol{W}_2 \delta (\boldsymbol{W}_1 \boldsymbol{z})) \\
\tilde{\boldsymbol{X}}_{:,:,c} = \boldsymbol{X}_{:,:,c} \circ s_c \quad \text{for} c=1,2,\dots,C</script><p>$F_{pool}$ 是全局池化操作，把每个通道聚合成一个标量 $\boldsymbol{z}_c$，$c$ 是通道的下标。用一个注意力机制对聚合的向量 $\boldsymbol{z}$ 使用非线性变换生成自适应的通道权重 $\boldsymbol{s}$，$\boldsymbol{W}_1, \boldsymbol{W}_2$ 是对应的权重，$\delta, \sigma$ 是 ReLU 和 sigmoid 激活函数。$\boldsymbol{s}$ 通过矩阵乘法乘到输入上。最后，输入通道基于学习到的权重得到了缩放。我们使用这个方法，针对一系列的图生成了时间依赖的注意力分数。</p>
<h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><h2 id="Region-level-ride-hailing-demand-forecasting"><a href="#Region-level-ride-hailing-demand-forecasting" class="headerlink" title="Region-level ride-hailing demand forecasting"></a>Region-level ride-hailing demand forecasting</h2><p>我们将城市分为相同大小的网格，每个格子定义为一个区域 $v \in V$，$V$ 表示城市内所有不相交的区域。$\boldsymbol{X}^{(t)}$ 表示第 $t$ 个时段所有区域的订单。<em>区域级别的网约车需求预测</em> 问题定义为：给定一个定长的输入，对单个时间步进行时空预测，也就是学习一个函数 $f: \mathbb{R}^{\vert V \vert \times T} \rightarrow \mathbb{R}^{\vert V \vert}$，将所有区域的历史需求映射到下一个时间步上。</p>
<script type="math/tex; mode=display">
[\boldsymbol{X}^{(t-T+1)}, \dots, \boldsymbol{X}^{(t)}]</script><p><strong>Framework overview</strong> ST-MGCN 的系统架构如图2。我们从不同的角度表示区域间的关系，顶点表示区域，边对区域间的关系编码。首先，我们使用提出的 CGRNN，考虑全局背景信息对不同时间的观测值进行聚合。然后，使用多图卷积捕获区域间不同类型的关系。最后，使用全连接神经网络将特征映射到预测上。</p>
<center><img src="/blog/2019/02/28/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Fig2.JPG" class title="Figure2"></center>

<h2 id="Spatial-dependency-modeling"><a href="#Spatial-dependency-modeling" class="headerlink" title="Spatial dependency modeling"></a>Spatial dependency modeling</h2><p>我们用图将区域间关系建模成三种类型，（1）邻居图 $\mathcal{G}_N = (V, \boldsymbol{A}_N)$，编码了空间相近程度，（2）功能相似度图 $\mathcal{G}_F = (V, \boldsymbol{A}_F)$，编码了区域的 POI 的相似度，（3）连接图 $\mathcal{G}_T = (V, \boldsymbol{A}_T)$，编码了距离较远的区域的连通性。我们的方法可以轻易地扩展到其他的图上。</p>
<p><strong>Neighborhood</strong> 区域的邻居基于空间近邻程度定义。我们将 $3 \times 3$ 区域中的最中间的那个区域与他邻接的 8 个区域相连。</p>
<script type="math/tex; mode=display">\tag{3}
A_{N, ij} = \begin{cases}
1, \quad v_i \quad \text{and} \quad v_j \quad \text{are} \quad \text{adjacent}\\
0, \quad \text{otherwise}
\end{cases}</script><p><strong>Functional similarity</strong> 对一个区域做预测的时候，很自然的会想到和这个区域在功能上相似的区域会有帮助。区域功能可以由 POI 刻画，两个顶点间的边定义为 POI 的相似度：</p>
<script type="math/tex; mode=display">\tag{3}
A_{S,i,j} = \text{sim}(P_{v_i}, P_{v_j}) \in [0, 1]</script><p>其中 $P_{v_i}, P_{v_j}$ 是区域 $v_i$ 和 $v_j$ 的 POI 向量，维度等于 POI 种类的个数，每个分量表示这个区域内这个 POI 类型的数量。</p>
<p><strong>Transportation connectivity</strong> 运输系统也是一个重要因素。一般来说，这些空间距离上相距较远但是可以很方便到达的区域可以关联起来。这种连接包含高速公路、公路、地铁这样的公共运输。我们定义：如果两个区域间通过这些路直接相连，那么他们之间有边：</p>
<script type="math/tex; mode=display">\tag{4}
A_{C,i,j} = max(0, \text{conn}(v_i, v_j) - A_{N,i,j}) \in \lbrace 0, 1\rbrace</script><p>$\text{conn}(u, v)$ 表示 $v_i$ 和 $v_j$ 之间的连通性。邻居的边在这个图中移除掉了，减少冗余的关系，所以这个图最后是一个稀疏图。</p>
<p><strong>Multi-graph convolution for spatial dependency modeling</strong> 有了这些图，我们提出了多图卷积对空间关系建模：</p>
<script type="math/tex; mode=display">\tag{5}
\boldsymbol{X}_{l+1} = \sigma(\bigsqcup_{\mathbf{A} \in \mathbb{A}} f(\mathbf{A; \theta_i}) \boldsymbol{X}_l \mathbf{W}_l)</script><p>其中 $\boldsymbol{X}_l \in \mathbb{R}^{\vert V \vert \times P_l}, \boldsymbol{X}_{l+1} \in \mathbb{R}^{\vert V \vert \times P_{l+1}}$ 是第 $l$ 和 $l+1$ 层的特征向量，$\sigma$ 是激活函数，$\bigsqcup$ 表示聚合函数，如 sum, max, average etc. $\mathbb{A}$ 表示图的集合，$f(\mathbf{A}; \theta_i) \in \mathbb{R}^{\vert V \vert \times \vert V \vert}$ 表示参数为 $\theta_i$ 的基于图 $\mathbf{A} \in \mathbb{A}$ 的不同样本组成的矩阵的聚合值，$\mathbf{W}_l \in \mathbb{R}^{P_l \times P_{l+1}}$ 表示特征变换矩阵，举个例子，如果 $f(\mathbf{A}, \theta_i)$ 是拉普拉斯矩阵 $\mathbf{L}$ 的多项式，那么这就是多图上的 ChebNet。如果是 $\mathbf{I}$，那就是全连接神经网络。</p>
<p>我们实现的是 $K$ 阶 拉普拉斯 $\mathbf{L}$ 多项式，图 3 是一个中心区域通过图卷积层变换后的例子。假设邻接矩阵中的值不是 0 就是 1，$L^k_{ij} \not = 0$ 表示 $v_i$ 在 $k$ 步内可达 $v_j$。根据卷积操作，$k$ 是空间特征提取时的感受野范围。使用图 1 的道路连通性图 $\mathcal{G}_C = (V, \boldsymbol{A}_C)$ 来说明。在邻接矩阵 $\boldsymbol{A}_C$ 中，我们有：</p>
<script type="math/tex; mode=display">
A_{C,1,4} = 1; A_{C,1,6} = 0; A_{C,4,6} = 1,</script><p>在 1 度拉普拉斯矩阵中对应的分量是：</p>
<script type="math/tex; mode=display">
L^1_{C,1,4} \not = 0; L^1_{C,1,6} = 0; L^1_{C,4,6} \not = 0</script><center><img src="/blog/2019/02/28/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Fig3.JPG" class title="Figure3"></center>

<p>如果拉普拉斯矩阵的最大度数 $K$ 设为 $1$，那么区域 1 变换的特征向量，即 $\boldsymbol{X}_{l+1, 1,:}$ 不会包含区域 6: $\boldsymbol{X}_{l,6,:}$，因为 $L^1_{C,1,6}=0$。当 $K$ 增大到 2 的时候，对应的元素 $L^2_{C,1,6}$ 变成非零，$\boldsymbol{X}_{l+1,1,:}$ 就可以利用 $\boldsymbol{X}_{l,6,:}$ 的信息了。</p>
<p>基于多图卷积的空间依赖建模不限于上述三种图，可以轻易地扩展到其他的图上，适用于其他的时空预测问题上。多图卷积对区域间的关系进行特征提取。感受野小的时候，专注于近邻的区域。增大拉普拉斯阶数，或者堆叠卷积层可以增加感受野的范围，鼓励模型捕获全局依赖关系。</p>
<p>图嵌入是另一种对区域间关系建模的方法。在 DMVST-Net (Yao et al. 2018b)，作者使用图嵌入表示区域间关系，然后将嵌入作为额外特征加到每个区域上。我们认为 ST-MGCN 中的空间依赖建模方法比之前的方法好，因为：ST-MGCN 将区域间关系编码到图中，通过图卷积从相关区域聚合需求值。但是在 DMVST-Net 中区域关系是嵌入到一个基于区域的不随时间变化的特征中，作为的模型的输入，</p>
<p>尽管 DMVST-Net 也捕获了拓扑结构信息，但是它很难从相关的区域中通过区域关系聚合需求值。而且不变的特征对模型训练的贡献有限。</p>
<h2 id="Temporal-correlation-modeling"><a href="#Temporal-correlation-modeling" class="headerlink" title="Temporal correlation modeling"></a>Temporal correlation modeling</h2><p>我们提出 Contextual Gated Recurrent Neural Network (CGRNN) 对不同时间步上的样本建模。CGRNN 通过使用一个上下文注意的门机制增强 RNN 将背景信息集成到时间建模中，结构如图 4。假设我们有 $T$ 个观测样本，$\boldsymbol{X}^{(t)} \in \mathbb{R}^{\vert V \vert \times P}$ 表示第 $t$ 个样本，$P$ 是特征数，如果特征只包含订单数，那就是 1。上下文门控机制如下：</p>
<script type="math/tex; mode=display">tag{6}
\hat{\boldsymbol{X}}^{(t)} = [\boldsymbol{X}^{(t)}, F^{K'}_\mathcal{G}(\boldsymbol{X}^{(t)})] \quad \text{for} \quad t = 1,2,\dots,T</script><p>首先，上下文门控机制通过将临近区域的历史信息和当前区域拼接，得到了区域的描述信息。从相邻区域来的信息看作是环境信息，通过图卷积 $F^{K’}_\mathcal{G}$ 使用最大阶数为 $K’$ 的拉普拉斯矩阵提取。上下文门控机制用来用图卷积操作集成临近区域的信息，然后使用一个池化：</p>
<script type="math/tex; mode=display">\tag{7}
z^{(t)} = F_{pool}(\hat{\boldsymbol{X}}^{(t)}) = \frac{1}{\vert V \vert} \sum^{\vert V \vert}_{i=1} \hat{X}^{(t)}_{i,:} \quad \text{for} \quad t=1,2,\dots,T</script><p>然后，我们在所有的区域上使用全局平均池化 $F_{pool}$ 生成每个时间步观测值的平均值。</p>
<script type="math/tex; mode=display">tag{8}
\boldsymbol{s} = \sigma(\boldsymbol{W}_2 \delta(\boldsymbol{W}_1) \boldsymbol{z})</script><p>然后使用一个注意力机制，$\boldsymbol{W}_1, \boldsymbol{W}_2$ 是参数，$\delta, \sigma$ 分别是 ReLU 和 sigmoid 激活。</p>
<script type="math/tex; mode=display">\tag{9}
\tilde{\boldsymbol{X}^{(t)}} = \boldsymbol{X}^{(t)} \circ s^{(t)} \quad \text{for} \quad t=1,2,\dots,T</script><p>最后，$\boldsymbol{s}$ 用来对每个时间样本进行缩放：</p>
<script type="math/tex; mode=display">tag{10}
\boldsymbol{H}_{i,:} = \text{RNN}(\tilde{\boldsymbol{X}}^{(1)}_{i,:}, \dots, \tilde{\boldsymbol{X}}^{(T)}_{i,:}; \boldsymbol{W}_3) \quad \text{for} \quad i=1,\dots,\vert V \vert</script><p>在上下文门控之后，使用一个共享的 RNN 对所有的区域进行计算，将每个区域聚合成单独的向量 $\boldsymbol{H}_{i,:}$。使用共享 RNN 的原因是我们想找到一个对所有区域通用的聚合规则，这个规则鼓励模型泛化且减少模型的复杂度。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p><strong>Dataset</strong> 北京和上海。时间是从2017年5月1日到2017年12月31日。5月1日到7月31日训练、8月1日到9月30日验证，剩下的测试。POI 数据是2017年的，包含13个类别。每个区域和一个 POI 向量相关，分量是这个 POI 类型在这个区域的个数。用来评估运输可达性的路网使用的是 OpenStreetMap (Haklay and Weber 2008)。</p>
<h2 id="Experimental-Settings"><a href="#Experimental-Settings" class="headerlink" title="Experimental Settings"></a>Experimental Settings</h2><p>学习任务是：$f: \mathbb{R}^{\vert V \vert \times T} \rightarrow \mathbb{R}^{\vert V \vert}$。实验中，我们将区域以 $1km \times 1km$ 的大小划分成网格。北京和上海分别 1296 和 896 个区域。就像 Zhang, Zheng, and Qi 2017 做的那样，网络的输入包含 5 个历史观测值，三个最近邻的部分，1个周期部分，一个最新的趋势部分。在构建运输可达性网络的时候，我们考虑了高速公路、公路、地铁。两个区域间只要有这样的路直接相连就认为是连通的。</p>
<p>$f(\mathbf{A}; \theta_i)$ 选择的是 $K = 2$ 时的切比雪夫多项式，$\bigsqcup$ 是 sum 函数。隐藏层为3，每层 64 个隐藏单元，L2 正则，weight decay 是 $1e-4$。CGRNN 中的图卷积 $K’$ 是 1。</p>
<p>我们使用 ReLU 作为图卷积的激活函数。ST-MGCN 的学习率是 $2e-3$，使用验证集上的早停。所有的算法都用 tf 实现，adam 优化 RMSE。ST-MGCN 训练时用了 10G 内存，9G GPU 显存。在 Tesla P40 单卡上训练了一个半小时。</p>
<p><strong>Methods for evaluation</strong> HA, LASSO, Ridge, Auto-regressive model(VAR, STAR), Gradient boosted machine (GBM), ST-ResNet (Zhang, Zheng and Qi 2017), DMVST-Net (Yao et al. 2018b), DCRNN, ST-GCN。</p>
<h2 id="Performance-comparison"><a href="#Performance-comparison" class="headerlink" title="Performance comparison"></a>Performance comparison</h2><p>我们在验证集上用网格搜索调整了所有模型的参数，在测试集上跑了多次得到的最后的结果。我们使用 RMSE 和 MAPE 作为评价指标。表 1 展示了不同方法在 10 次以上的预测中的对比结果。</p>
<center><img src="/blog/2019/02/28/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Table1.JPG" class title="Table1"></center>

<p>我们在两个数据集上观测到了几个现象：（1）基于深度学习的方法能够对非线性的时空依赖关系建模，比其他的方法好；（2）ST-MGCN 在两个数据集上比其他的方法都好，比第二好的高出 10%；（3）对比其他的深度学习方法，ST-MGCN 的方差更小。</p>
<h2 id="Effect-of-spatial-dependency-modeling"><a href="#Effect-of-spatial-dependency-modeling" class="headerlink" title="Effect of spatial dependency modeling"></a>Effect of spatial dependency modeling</h2><p>为了研究空间和时间依赖建模的效果，我们通过减少模型中的组成部分评估了 ST-MGCN 的几个变体，包括：（1）邻居图，（2）功能相似性图，（3）运输连通性图。结果如表 2 所示。移除任何一个图都会造成性能损失，证明了每种关系的重要性。这些图编码了重要的先验知识，也就是区域间的相关性。</p>
<center><img src="/blog/2019/02/28/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Table2.JPG" class title="Table2"></center>

<p>为了评估集成多个区域关系的效果，我们扩展了基于单个图的模型，包括 DCRNN 和 STGCN，分别记为 DCRNN+ 和 ST-GCN+。结果如图 3，两个算法都得到了提升。</p>
<center><img src="/blog/2019/02/28/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Table3.JPG" class title="Table3"></center>

<h2 id="Effect-of-temporal-dependency-modeling"><a href="#Effect-of-temporal-dependency-modeling" class="headerlink" title="Effect of temporal dependency modeling"></a>Effect of temporal dependency modeling</h2><p>我们使用不同的方法对时间建模，评估 ST-GCN 对时间关系建模的效果。（1）平均池化：通过平均池化对历史观测值进行聚合，（2）RNN：使用 RNN 对历史观测值聚合，（3）CG：使用上下文门对不同的历史观测值赋权，不适用 RNN，（4）GRNN：不用图卷积的 CGRNN。结果如表 4。我们观察到了以下现象：</p>
<ul>
<li>平均池化会盲目地平均不同的样本，导致性能下降，能做上下文依赖非线性时间聚合的 RNN 能显著地提升性能。</li>
<li>CGRNN 增强了 RNN。移除 RNN 和 图卷积都导致性能下降，证明了每个部件的有效性。</li>
</ul>
<center><img src="/blog/2019/02/28/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Table4.JPG" class title="Table4"></center>

<h2 id="Effect-of-model-parameters"><a href="#Effect-of-model-parameters" class="headerlink" title="Effect of model parameters"></a>Effect of model parameters</h2><p>我们调整了两个最重要的参数来看不同参数对模型的影响，$K$ 和图卷积层数。图 5 展示了测试集上的结果。可以观察到随着层数的增加，错误先降后增。但是随着 $K$ 的增加，错误是先减小，后不变。越大的 $K$ 或层数使得模型能捕获全局关联性，代价是模型的复杂度会增加，更易过拟合。</p>
<center><img src="/blog/2019/02/28/spatiotemporal-multi-graph-convolution-network-for-ride-hailing-demand-forecasting/Fig5.JPG" class title="Figure5"></center>

<h1 id="Conclusion-and-Future-work"><a href="#Conclusion-and-Future-work" class="headerlink" title="Conclusion and Future work"></a>Conclusion and Future work</h1><p>我们研究的是网约车需求预测，要找寻这个问题唯一的时空依赖关系。我们提出的深度学习模型使用多个图对区域间的非欧关系建模，使用多图卷积明显的捕获了这个关系。然后用上下文门控机制增强了 RNN，在时间建模上集成了全局背景信息。在两个大型真实数据集上评估了模型，比 state-of-the-art好。未来的工作是：（1）在其他的时空预测任务上评估模型；（2）将提出的模型扩展到多步预测上。</p>
</div><div class="article-tags size-small is-uppercase mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/blog/tags/graph/">Graph</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/deep-learning/">deep learning</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/graph-convolutional-network/">graph convolutional network</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/spatial-temporal/">Spatial-temporal</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/time-series/">Time Series</a></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button is-info donate"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/blog/img/alipay.jpeg" alt="Alipay"></span></a><a class="button is-success donate"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/blog/img/wechat.jpeg" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/blog/2019/03/07/t-gcn-a-temporal-graph-convolutional-network-for-traffic-prediction/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">T-GCN: A Temporal Graph Convolutional Network for Traffic Prediction</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/blog/2019/02/04/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F%E6%94%B9%E8%BF%9B/"><span class="level-item">归并排序改进</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="is-rounded" src="/blog/img/avatar.png" alt="Davidham"></figure><p class="title is-size-4 is-block line-height-inherit">Davidham</p><p class="is-size-6 is-block">阿里菜鸟-时空数据挖掘-算法工程师</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Hangzhou</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/blog/archives"><p class="title">81</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/blog/categories"><p class="title">8</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/blog/tags"><p class="title">34</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/davidham3" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/davidham3"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="mail" href="mailto:mengxian.sc@alibaba-inc.com"><i class="fa fa-envelope"></i></a></div></div></div><!--!--><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/blog/categories/algorithms/"><span class="level-start"><span class="level-item">algorithms</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/dataset/"><span class="level-start"><span class="level-item">dataset</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/software/"><span class="level-start"><span class="level-item">software</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B3%E5%8F%B0/"><span class="level-start"><span class="level-item">分布式平台</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">机器学习</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"><span class="level-start"><span class="level-item">自然语言处理</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文阅读笔记</span></span><span class="level-end"><span class="level-item tag">46</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E9%9A%8F%E7%AC%94/"><span class="level-start"><span class="level-item">随笔</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/blog/tags/attention/"><span class="tag">Attention</span><span class="tag is-grey-lightest">5</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/graph/"><span class="tag">Graph</span><span class="tag is-grey-lightest">27</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/hadoop/"><span class="tag">Hadoop</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/kafka/"><span class="tag">Kafka</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/ner/"><span class="tag">NER</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/resnet/"><span class="tag">ResNet</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/sequence/"><span class="tag">Sequence</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/spark/"><span class="tag">Spark</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/spatial-temporal/"><span class="tag">Spatial-temporal</span><span class="tag is-grey-lightest">15</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/time-series/"><span class="tag">Time Series</span><span class="tag is-grey-lightest">12</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/algorithms/"><span class="tag">algorithms</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/computer-vision/"><span class="tag">computer vision</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/dataset/"><span class="tag">dataset</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/deep-learning/"><span class="tag">deep learning</span><span class="tag is-grey-lightest">44</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/event-sequence/"><span class="tag">event sequence</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/graph-convolutional-network/"><span class="tag">graph convolutional network</span><span class="tag is-grey-lightest">20</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/image-style-transfer/"><span class="tag">image style transfer</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/implicit-feedback/"><span class="tag">implicit feedback</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/language-modeling/"><span class="tag">language modeling</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/large-scale-learning/"><span class="tag">large-scale learning</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/machine-learning/"><span class="tag">machine learning</span><span class="tag is-grey-lightest">19</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/machine-translation/"><span class="tag">machine translation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/natural-language-processing/"><span class="tag">natural language processing</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/normalization/"><span class="tag">normalization</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/recommender-system/"><span class="tag">recommender system</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/reinforcement-learning/"><span class="tag">reinforcement learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/seq2seq/"><span class="tag">seq2seq</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/software/"><span class="tag">software</span><span class="tag is-grey-lightest">16</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/softwares/"><span class="tag">softwares</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/super-resolution/"><span class="tag">super resolution</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/virtual-machine/"><span class="tag">virtual machine</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/vscode/"><span class="tag">vscode</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E5%B7%B2%E5%A4%8D%E7%8E%B0/"><span class="tag">已复现</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E9%9A%8F%E7%AC%94/"><span class="tag">随笔</span><span class="tag is-grey-lightest">1</span></a></div></div></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><h3 class="menu-label">Recent</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2022-04-27T15:37:58.000Z">2022-04-27</time></p><p class="title is-6"><a class="link-muted" href="/blog/2022/04/27/time-dependent-representation-for-neural-event-sequence-prediction/">Time-dependent representation for neural event sequence prediction</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2022-04-26T14:38:05.000Z">2022-04-26</time></p><p class="title is-6"><a class="link-muted" href="/blog/2022/04/26/event-sequence-metric-learning/">Event sequence metric learning</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-05-14T04:00:28.000Z">2020-05-14</time></p><p class="title is-6"><a class="link-muted" href="/blog/2020/05/14/%E5%8D%9A%E5%AE%A2%E9%87%8D%E8%A3%85/">博客重装...</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/software/">software</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-05-13T09:07:14.000Z">2020-05-13</time></p><p class="title is-6"><a class="link-muted" href="/blog/2020/05/13/neural-collaborative-filtering/">Neural Collaborative Filtering</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-02-12T16:12:07.000Z">2020-02-13</time></p><p class="title is-6"><a class="link-muted" href="/blog/2020/02/13/%E8%AE%B0%E4%B8%80%E6%AC%A1pyspark%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87%EF%BC%8Cnp-frombuffer%E7%9A%84%E4%BD%BF%E7%94%A8/">记一次pyspark性能提升，np.frombuffer的使用</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B3%E5%8F%B0/">分布式平台</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/blog/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2020/05/"><span class="level-start"><span class="level-item">May 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2020/02/"><span class="level-start"><span class="level-item">February 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2020/01/"><span class="level-start"><span class="level-item">January 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/12/"><span class="level-start"><span class="level-item">December 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/11/"><span class="level-start"><span class="level-item">November 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/08/"><span class="level-start"><span class="level-item">August 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/07/"><span class="level-start"><span class="level-item">July 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/06/"><span class="level-start"><span class="level-item">June 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/05/"><span class="level-start"><span class="level-item">May 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/04/"><span class="level-start"><span class="level-item">April 2019</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/03/"><span class="level-start"><span class="level-item">March 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/02/"><span class="level-start"><span class="level-item">February 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/01/"><span class="level-start"><span class="level-item">January 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/12/"><span class="level-start"><span class="level-item">December 2018</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/10/"><span class="level-start"><span class="level-item">October 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/09/"><span class="level-start"><span class="level-item">September 2018</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/08/"><span class="level-start"><span class="level-item">August 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/07/"><span class="level-start"><span class="level-item">July 2018</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/06/"><span class="level-start"><span class="level-item">June 2018</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/05/"><span class="level-start"><span class="level-item">May 2018</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/04/"><span class="level-start"><span class="level-item">April 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/03/"><span class="level-start"><span class="level-item">March 2018</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/02/"><span class="level-start"><span class="level-item">February 2018</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/01/"><span class="level-start"><span class="level-item">January 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2017/08/"><span class="level-start"><span class="level-item">August 2017</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/"><img src="/blog/img/logo.svg" alt="Davidham&#039;s blog" height="28"></a><p class="size-small"><span>&copy; 2022 Davidham</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://davidham3.github.io',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/blog/js/back_to_top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>