<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>pyspark读写HBase - Davidham&#039;s blog</title><meta description="应甲方需求，写一个 pyspark 读写 HBase 的教程。主要包含了基本读写方法和自定义 Converter 的方法。"><meta property="og:type" content="blog"><meta property="og:title" content="pyspark读写HBase"><meta property="og:url" content="https://davidham3.github.io/2019/04/10/pyspark%E8%AF%BB%E5%86%99hbase/"><meta property="og:site_name" content="Davidham&#039;s blog"><meta property="og:description" content="应甲方需求，写一个 pyspark 读写 HBase 的教程。主要包含了基本读写方法和自定义 Converter 的方法。"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://davidham3.github.io/blog/img/og_image.png"><meta property="article:published_time" content="2019-04-10T09:26:30.000Z"><meta property="article:modified_time" content="2022-04-25T14:58:30.788Z"><meta property="article:author" content="Davidham"><meta property="article:tag" content="software"><meta property="article:tag" content="Spark"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/blog/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://davidham3.github.io/2019/04/10/pyspark%E8%AF%BB%E5%86%99hbase/"},"headline":"Davidham's blog","image":["https://davidham3.github.io/blog/img/og_image.png"],"datePublished":"2019-04-10T09:26:30.000Z","dateModified":"2022-04-25T14:58:30.788Z","author":{"@type":"Person","name":"Davidham"},"description":"应甲方需求，写一个 pyspark 读写 HBase 的教程。主要包含了基本读写方法和自定义 Converter 的方法。"}</script><link rel="canonical" href="https://davidham3.github.io/2019/04/10/pyspark%E8%AF%BB%E5%86%99hbase/"><link rel="icon" href="/blog/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/"><img src="/blog/img/logo.svg" alt="Davidham&#039;s blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/blog/">Home</a><a class="navbar-item" href="/blog/archives">Archives</a><a class="navbar-item" href="/blog/categories">Categories</a><a class="navbar-item" href="/blog/tags">Tags</a><a class="navbar-item" href="/blog/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2019-04-10T09:26:30.000Z" title="2019-04-10T09:26:30.000Z">2019-04-10</time><span class="level-item"><a class="link-muted" href="/blog/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B3%E5%8F%B0/">分布式平台</a></span><span class="level-item">23 minutes read (About 3397 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">pyspark读写HBase</h1><div class="content"><p>应甲方需求，写一个 pyspark 读写 HBase 的教程。主要包含了基本读写方法和自定义 Converter 的方法。<br><a id="more"></a></p>
<h1 id="pyspark-读取-HBase"><a href="#pyspark-读取-HBase" class="headerlink" title="pyspark 读取 HBase"></a>pyspark 读取 HBase</h1><p>以下内容的环境：python 3.5，spark 1.6</p>
<p>pyspark 读取 HBase 需要借助 Java 的类完成读写。</p>
<p>首先需要明确的是，HBase 中存储的是 <code>byte[]</code>，也就是说，不管是什么样的数据，都需要先转换为 <code>byte[]</code> 后，才能存入 HBase。</p>
<h2 id="基本方法"><a href="#基本方法" class="headerlink" title="基本方法"></a>基本方法</h2><p>pyspark 读取 HBase 需要使用 <code>SparkContext</code> 的 <a href="http://spark.apache.org/docs/1.6.0/api/python/pyspark.html#pyspark.SparkContext.newAPIHadoopRDD">newAPIHadoopRDD</a> 这个方法，这个方法需要使用 Java 的类，用这些类读取 HBase</p>
<p>下面的示例代码默认 HBase 中的行键、列族名、列名和值都是字符串转成的 <code>byte</code> 数组：</p>
<p>read_hbase_pyspark.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    conf = SparkConf().set(<span class="string">"spark.executorEnv.PYTHONHASHSEED"</span>, <span class="string">"0"</span>)\</span><br><span class="line">                      .set(<span class="string">"spark.kryoserializer.buffer.max"</span>, <span class="string">"2040mb"</span>)</span><br><span class="line">    sc = SparkContext(appName=<span class="string">'HBaseInputFormat'</span>, conf=conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 配置项要包含 zookeeper 的 ip</span></span><br><span class="line">    zookeeper_host = <span class="string">'zkServer'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 还要包含要读取的 HBase 表名</span></span><br><span class="line">    hbase_table_name = <span class="string">'testTable'</span>    </span><br><span class="line"></span><br><span class="line">    conf = &#123;<span class="string">"hbase.zookeeper.quorum"</span>: zookeeper_host, <span class="string">"hbase.mapreduce.inputtable"</span>: hbase_table_name&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这个Java类用来将 HBase 的行键转换为字符串</span></span><br><span class="line">    keyConv = <span class="string">"org.apache.spark.examples.pythonconverters.ImmutableBytesWritableToStringConverter"</span></span><br><span class="line">    <span class="comment"># 这个Java类用来将 HBase 查询得到的结果，转换为字符串</span></span><br><span class="line">    valueConv = <span class="string">"org.apache.spark.examples.pythonconverters.HBaseResultToStringConverter"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第一个参数是 hadoop 文件的输入类型</span></span><br><span class="line">    <span class="comment"># 第二个参数是 HBase rowkey 的类型</span></span><br><span class="line">    <span class="comment"># 第三个参数是 HBase 值的类型</span></span><br><span class="line">    <span class="comment"># 这三个参数不用改变</span></span><br><span class="line">    <span class="comment"># 读取后的 rdd，每个元素是一个键值对，(key, value)</span></span><br><span class="line">    hbase_rdd = sc.newAPIHadoopRDD(</span><br><span class="line">        <span class="string">"org.apache.hadoop.hbase.mapreduce.TableInputFormat"</span>,</span><br><span class="line">        <span class="string">"org.apache.hadoop.hbase.io.ImmutableBytesWritable"</span>,</span><br><span class="line">        <span class="string">"org.apache.hadoop.hbase.client.Result"</span>,</span><br><span class="line">        keyConverter=keyConv,</span><br><span class="line">        valueConverter=valueConv,</span><br><span class="line">        conf=conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读取后，将键值对 (key, value) 中的值 value，使用\n切分，用 flatMap 展开</span></span><br><span class="line">    <span class="comment"># 然后将键值对 (key, value) 中的值 value 使用 json.loads 解析，得到 dict</span></span><br><span class="line">    hbase_rdd = hbase_rdd.flatMapValues(<span class="keyword">lambda</span> v: v.split(<span class="string">"\n"</span>)).mapValues(json.loads)</span><br><span class="line"></span><br><span class="line">    output = hbase_rdd.collect()</span><br><span class="line">    <span class="keyword">for</span> (k, v) <span class="keyword">in</span> output:</span><br><span class="line">        print((k, v))</span><br></pre></td></tr></table></figure></p>
<p>上述代码在提交给 spark 集群的时候，要指名用到的 Java 类的位置，这些类都在 spark-examples 这个包里面，这个包在 spark 目录下的 lib 里面。以 CDH 5.7.2 为例，CDH 集群中这个包的位置在 <code>/opt/cloudera/parcels/CDH-5.7.2-1.cdh5.7.2.p0.18/lib/spark/lib/spark-examples-1.6.0-cdh5.7.2-hadoop2.6.0-cdh5.7.2.jar</code>，所以提交命令为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --master yarn --jars /opt/cloudera/parcels/CDH-5.7.2-1.cdh5.7.2.p0.18/lib/spark/lib/spark-examples-1.6.0-cdh5.7.2-hadoop2.6.0-cdh5.7.2.jar read_hbase_pyspark.py</span><br></pre></td></tr></table></figure>
<p>所以，上述的 Java 类，核心都是认为 HBase 中所有的值，原本都是字符串，然后转换成 <code>byte</code> 数组后存入的 HBase，它在解析的时候，将读取到的 <code>byte[]</code> 转换为字符串后返回，所以我们拿到的值就是字符串。</p>
<h2 id="进阶方法"><a href="#进阶方法" class="headerlink" title="进阶方法"></a>进阶方法</h2><p>对于其他类型的数据，转换为 <code>byte</code> 数组后存入 HBase，如果我们还使用上面的 Java 类去读取 HBase，那么我们拿到的字符串的值就是不正确的。</p>
<p>为了理解这些内容，我们首先要讨论 HBase 中值的存储结构。</p>
<p>HBase 是非结构化数据库，以行为单位，每行拥有一个行键 rowkey，对应的值可以表示为一个 map（python 中的 dict），举个例子，如果我们有一条记录，行键记为 “r1”，里面有 1 个列族(columnFamily) “A”，列族中有两列(qualifier)，分别记为 “a” 和 “b”，对应的值分别为 “v1” 和 “v2”，那么表示成 json 字符串就是下面的形式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"r1"</span>: &#123;</span><br><span class="line">        <span class="string">"A"</span> : &#123;</span><br><span class="line">            <span class="string">"a"</span>: <span class="string">"v1"</span>,</span><br><span class="line">            <span class="string">"b"</span>: <span class="string">"v2"</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面这个 json 字符串就是上面那条记录在 HBase 中存储的示例，第一层的键表示行键(rowkey)，对应的值表示这一行的值；第二层的键表示列族名(columnFamily)，值表示这个列族下列的值；第三层的键表示列名(qualifier)，对应的值(value)表示这个由行键、列族名、列名三项确定的一个单元格(Cell)内的值。所以上面这个例子中，只有一行，两个单元格。</p>
<p>下面我们针对 pyspark 读取 HBase 使用到的 <code>org.apache.spark.examples.pythonconverters.HBaseResultToStringConverter</code> 来讨论。</p>
<p>Java 的 API 在读取 HBase 的时候，会得到一个 <code>Result</code> 类型，这个 <code>Result</code> 就是查询结果。<code>Result</code> 可以遍历，里面拥有多个 <code>Cell</code>，也就是单元格。上面我们说了，每个单元格至少有 4 个内容：行键、列族名、列名、值。</p>
<p><code>HBaseResultToStringConverter</code> 是由 scala 实现的一个类，它的功能是将 Java HBase API 的 <code>Result</code> 转换为 <code>String</code>，源码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.spark.examples.pythonconverters</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConverters</span>._</span><br><span class="line"><span class="keyword">import</span> scala.util.parsing.json.<span class="type">JSONObject</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.python.<span class="type">Converter</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.&#123;<span class="type">Put</span>, <span class="type">Result</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.<span class="type">Bytes</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.<span class="type">KeyValue</span>.<span class="type">Type</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.<span class="type">CellUtil</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HBaseResultToStringConverter</span> <span class="keyword">extends</span> <span class="title">Converter</span>[<span class="type">Any</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">convert</span></span>(obj: <span class="type">Any</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> result = obj.asInstanceOf[<span class="type">Result</span>]</span><br><span class="line">    <span class="keyword">val</span> output = result.listCells.asScala.map(cell =&gt;</span><br><span class="line">        <span class="type">Map</span>(</span><br><span class="line">          <span class="string">"row"</span> -&gt; <span class="type">Bytes</span>.toStringBinary(<span class="type">CellUtil</span>.cloneRow(cell)),</span><br><span class="line">          <span class="string">"columnFamily"</span> -&gt; <span class="type">Bytes</span>.toStringBinary(<span class="type">CellUtil</span>.cloneFamily(cell)),</span><br><span class="line">          <span class="string">"qualifier"</span> -&gt; <span class="type">Bytes</span>.toStringBinary(<span class="type">CellUtil</span>.cloneQualifier(cell)),</span><br><span class="line">          <span class="string">"timestamp"</span> -&gt; cell.getTimestamp.toString,</span><br><span class="line">          <span class="string">"type"</span> -&gt; <span class="type">Type</span>.codeToType(cell.getTypeByte).toString,</span><br><span class="line">          <span class="string">"value"</span> -&gt; <span class="type">Bytes</span>.toStringBinary(<span class="type">CellUtil</span>.cloneValue(cell))</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line">    output.map(<span class="type">JSONObject</span>(_).toString()).mkString(<span class="string">"\n"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>它完成的工作是遍历 <code>Result</code> 中的 <code>Cell</code>，每个 <code>Cell</code> 转换成一个 scala <code>Map</code>，键分别是行键、列族名、列名、时间戳、HBase 操作类型、值。最后每个 scala <code>Map</code> 被转换成 json 字符串，之间用 ‘\n’ 分隔。</p>
<p>这里的 <code>CellUtil.CloneRow</code>，<code>CellUtil.cloneFamily</code>，<code>CellUtil.cloneQualifier</code>，<code>CellUtil.cloneValue</code> 是我们主要使用的四个方法，这四个方法生成的都是 <code>byte[]</code>，然后这四个 <code>byte[]</code> 都被 <code>Bytes.toStringBinary</code> 转换成了 <code>String</code> 类型。</p>
<p>所以，如果我们存入 HBase 的数据是 <code>String</code> 以外类型的，如 <code>Float</code>, <code>Double</code>, <code>BigDecimal</code>，那么这里使用 <code>CellUtil</code> 的方法拿到 <code>byte[]</code> 后，需要使用 <code>Bytes</code> 里面的对应方法转换为原来的类型，再转成字符串或其他类型，生成 json 字符串，然后返回，这样我们通过 pyspark 才能拿到正确的值。</p>
<p>下面是一个示例，我们的数据都是 <code>java.math.BigDecimal</code> 类型的值，存 HBase 的时候将他们转换为 <code>byte[]</code> 后进行了存储。那么解析的时候，就需要自定义一个处理 <code>BigDecimal</code> 的类：<code>HBaseResultToBigDecimalToStringConverter</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.spark.examples.pythonconverters</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.math.<span class="type">BigDecimal</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConverters</span>._</span><br><span class="line"><span class="keyword">import</span> scala.util.parsing.json.<span class="type">JSONObject</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.python.<span class="type">Converter</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.&#123;<span class="type">Put</span>, <span class="type">Result</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.<span class="type">ImmutableBytesWritable</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.<span class="type">Bytes</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.<span class="type">KeyValue</span>.<span class="type">Type</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.<span class="type">CellUtil</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HBaseResultToBigDecimalToStringConverter</span> <span class="keyword">extends</span> <span class="title">Converter</span>[<span class="type">Any</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">convert</span></span>(obj: <span class="type">Any</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> result = obj.asInstanceOf[<span class="type">Result</span>]</span><br><span class="line">    <span class="keyword">val</span> output = result.listCells.asScala.map(cell =&gt;</span><br><span class="line">        <span class="type">Map</span>(</span><br><span class="line">          <span class="string">"row"</span> -&gt; <span class="type">Bytes</span>.toStringBinary(<span class="type">CellUtil</span>.cloneRow(cell)),</span><br><span class="line">          <span class="string">"columnFamily"</span> -&gt; <span class="type">Bytes</span>.toStringBinary(<span class="type">CellUtil</span>.cloneFamily(cell)),</span><br><span class="line">          <span class="string">"qualifier"</span> -&gt; <span class="type">Bytes</span>.toStringBinary(<span class="type">CellUtil</span>.cloneQualifier(cell)),</span><br><span class="line">          <span class="string">"timestamp"</span> -&gt; cell.getTimestamp.toString,</span><br><span class="line">          <span class="string">"type"</span> -&gt; <span class="type">Type</span>.codeToType(cell.getTypeByte).toString,</span><br><span class="line">          <span class="string">"value"</span> -&gt; <span class="type">Bytes</span>.toBigDecimal(<span class="type">CellUtil</span>.cloneValue(cell)).toString()</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line">    output.map(<span class="type">JSONObject</span>(_).toString()).mkString(<span class="string">"\n"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上述代码中，引入了 <code>java.math.BigDecimal</code>，将 <code>value</code> 的解析进行了简单的修改，通过 <code>CellUtil.cloneValue</code> 拿到 <code>byte[]</code> 后，通过 <code>Bytes.toBigDecimal</code> 转换成 <code>java.math.BigDecimal</code>，然后使用 <code>toString</code> 方法转换成字符串。</p>
<p>这个类写完后，我们就可以对其进行编译，导出成 jar 包，在 pyspark 程序中指明，读取的时候，使用这个类解析 value。</p>
<p>这样源代码就改完了，需要编译成 jar 包。</p>
<p>首先安装 <a href="http://maven.apache.org/">maven</a> 3.6.0，下载后，解压，配置环境变量即可。</p>
<p>下载 spark 的源码，去 Apache Spark 官网，下载仓库中的源代码 <a href="https://archive.apache.org/dist/spark/spark-1.6.0/">spark-1.6.0.tgz</a> 。</p>
<p>下载后解压，将根目录中的 scalastyle-config.xml 拷贝到 examples 目录下。</p>
<p>修改 <code>examples/src/main/scala/org/apache/spark/examples/pythonconverters/HBaseConverters.scala</code>，增加自己用的类。</p>
<p>修改 <code>examples/pom.xml</code>，将 <code>&lt;artifactId&gt;spark-examples_2.10&lt;/artifactId&gt;</code> 修改为 <code>&lt;artifactId&gt;spark-examples_2.10_my_converters&lt;/artifactId&gt;</code>。</p>
<p>cd 到 examples 目录下，使用以下命令编译 spark-examples</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean install -pl :spark-examples_2.10_my_converters</span><br></pre></td></tr></table></figure>
<p>编译途中保证全程联网，编译的时候会有一些警告，编译好的包在同级目录下的 target 中，有个叫 spark-examples_2.10_my_converters-1.6.0.jar 的文件。</p>
<p>然后就是使用这个包读取 HBase 中的 BigDecimal了：</p>
<p>我们使用 standalone 模式运行 pyspark 交互式界面：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyspark --master spark://host1:7077 --jars spark-examples_2.10_my_converters-1.6.0.jar</span><br></pre></td></tr></table></figure>
<p>执行以下内容：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">zookeeper_host = <span class="string">'host1'</span></span><br><span class="line">hbase_table_name = <span class="string">'testTable'</span></span><br><span class="line"></span><br><span class="line">conf = &#123;<span class="string">"hbase.zookeeper.quorum"</span>: zookeeper_host, <span class="string">"hbase.mapreduce.inputtable"</span>: hbase_table_name&#125;</span><br><span class="line">keyConv = <span class="string">"org.apache.spark.examples.pythonconverters.ImmutableBytesWritableToStringConverter"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意这里，使用自己定义的Converter读取</span></span><br><span class="line">valueConv = <span class="string">"org.apache.spark.examples.pythonconverters.HBaseResultToBigDecimalToStringConverter"</span></span><br><span class="line"></span><br><span class="line">hbase_rdd = sc.newAPIHadoopRDD(</span><br><span class="line">        <span class="string">"org.apache.hadoop.hbase.mapreduce.TableInputFormat"</span>,</span><br><span class="line">        <span class="string">"org.apache.hadoop.hbase.io.ImmutableBytesWritable"</span>,</span><br><span class="line">        <span class="string">"org.apache.hadoop.hbase.client.Result"</span>,</span><br><span class="line">        keyConverter=keyConv,</span><br><span class="line">        valueConverter=valueConv,</span><br><span class="line">        conf=conf)</span><br><span class="line">hbase_rdd = hbase_rdd.flatMapValues(<span class="keyword">lambda</span> v: v.split(<span class="string">"\n"</span>)).mapValues(json.loads)</span><br><span class="line"></span><br><span class="line">hbase_rdd.take(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>然后就可以看到结果了，如何验证读取的对不对呢，可以尝试将 <code>valueConv</code> 改回 <code>HBaseResultToStringConverter</code>，然后观察 value 的值。</p>
<p>以上就是如何通过修改 HBaseConverters.scala 让 pyspark 从 HBase 中读取 <code>java.math.BigDecimal</code> 的示例。</p>
<h1 id="pyspark-写入-HBase"><a href="#pyspark-写入-HBase" class="headerlink" title="pyspark 写入 HBase"></a>pyspark 写入 HBase</h1><p>pyspark 写入 HBase 使用 <code>SparkContext</code> 的 <a href="http://spark.apache.org/docs/1.6.0/api/python/pyspark.html#pyspark.RDD.saveAsNewAPIHadoopDataset">saveAsNewAPIHadoopDataset</a>，和读取的方法类似，也需要使用 Java 的类。</p>
<p><strong>下面的方法要求存入 HBase 中的数据，行键、列族名、列名、值都为字符串</strong></p>
<p>write_into_hbase_pyspark.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    conf = SparkConf().set(<span class="string">"spark.executorEnv.PYTHONHASHSEED"</span>, <span class="string">"0"</span>)\</span><br><span class="line">                      .set(<span class="string">"spark.kryoserializer.buffer.max"</span>, <span class="string">"2040mb"</span>)</span><br><span class="line">    sc = SparkContext(appName=<span class="string">'HBaseOutputFormat'</span>, conf=conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 配置项要包含 zookeeper 的 ip</span></span><br><span class="line">    zookeeper_host = <span class="string">'zkServer'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 还要包含要写入的 HBase 表名</span></span><br><span class="line">    hbase_table_name = <span class="string">'testTable'</span>    </span><br><span class="line"></span><br><span class="line">    conf = &#123;<span class="string">"hbase.zookeeper.quorum"</span>: zookeeper_host,</span><br><span class="line">            <span class="string">"hbase.mapred.outputtable"</span>: hbase_table_name,</span><br><span class="line">            <span class="string">"mapreduce.outputformat.class"</span>: <span class="string">"org.apache.hadoop.hbase.mapreduce.TableOutputFormat"</span>,</span><br><span class="line">            <span class="string">"mapreduce.job.output.key.class"</span>: <span class="string">"org.apache.hadoop.hbase.io.ImmutableBytesWritable"</span>,</span><br><span class="line">            <span class="string">"mapreduce.job.output.value.class"</span>: <span class="string">"org.apache.hadoop.io.Writable"</span>&#125;</span><br><span class="line">    keyConv = <span class="string">"org.apache.spark.examples.pythonconverters.StringToImmutableBytesWritableConverter"</span></span><br><span class="line">    valueConv = <span class="string">"org.apache.spark.examples.pythonconverters.StringListToPutConverter"</span></span><br><span class="line"></span><br><span class="line">    records = [</span><br><span class="line">        [<span class="string">'row1'</span>, <span class="string">'f1'</span>, <span class="string">'q1'</span>, <span class="string">'value1'</span>],</span><br><span class="line">        [<span class="string">'row2'</span>, <span class="string">'f1'</span>, <span class="string">'q1'</span>, <span class="string">'value2'</span>],</span><br><span class="line">        [<span class="string">'row3'</span>, <span class="string">'f1'</span>, <span class="string">'q1'</span>, <span class="string">'value3'</span>],</span><br><span class="line">        [<span class="string">'row4'</span>, <span class="string">'f1'</span>, <span class="string">'q1'</span>, <span class="string">'value4'</span>]</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    sc.parallelize(records)\</span><br><span class="line">      .map(<span class="keyword">lambda</span> x: (x[<span class="number">0</span>], x))\</span><br><span class="line">      .saveAsNewAPIHadoopDataset(</span><br><span class="line">        conf=conf,</span><br><span class="line">        keyConverter=keyConv,</span><br><span class="line">        valueConverter=valueConv)</span><br></pre></td></tr></table></figure></p>
<p>首先在控制台启动 HBase-shell</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase shell</span><br></pre></td></tr></table></figure>
<p>然后创建表，表名为 testTable，只有一个列族，列族名为 f1：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create <span class="string">'testTable'</span>, <span class="string">'f1'</span></span><br></pre></td></tr></table></figure>
<p>使用 <code>quit</code> 退出 HBase-shell</p>
<p>提交 pyspark 程序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --master spark:&#x2F;&#x2F;master:7077 --jars &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-5.7.2-1.cdh5.7.2.p0.18&#x2F;lib&#x2F;spark&#x2F;lib&#x2F;spark-examples-1.6.0-cdh5.7.2-hadoop2.6.0-cdh5.7.2.jar write_into_hbase_pyspark.py</span><br></pre></td></tr></table></figure>
<p>运行完成后，再次进入 HBase-shell，运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scan <span class="string">'testTable'</span></span><br></pre></td></tr></table></figure>
<p>可以看到类似下面的输出结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; scan &#39;testTable&#39;</span><br><span class="line">ROW                           COLUMN+CELL</span><br><span class="line"> row1                         column&#x3D;f1:q1, timestamp&#x3D;1554892784494, value&#x3D;value1</span><br><span class="line"> row2                         column&#x3D;f1:q1, timestamp&#x3D;1554892784494, value&#x3D;value2</span><br><span class="line"> row3                         column&#x3D;f1:q1, timestamp&#x3D;1554892816961, value&#x3D;value3</span><br><span class="line"> row4                         column&#x3D;f1:q1, timestamp&#x3D;1554892816961, value&#x3D;value4</span><br><span class="line">4 row(s) in 0.3330 seconds</span><br></pre></td></tr></table></figure>
<p>这就完成了写入 HBase 的过程。</p>
<p><strong>需要注意的是：rdd 中的每个元素，都必须是一个列表(<code>list</code>)，不能是其他类型，如 <code>tuple</code>，而且每个列表内必须是 4 个元素，分别表示 <code>[行键、列族名、列名、值]</code>，且每个元素都为 <code>str</code> 类型。</strong></p>
<p>原因是 <code>StringListToPutConverter</code> 这个类做转换的时候需要将 rdd 中的元素，看作是一个 <code>java.util.ArrayList[String]</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StringListToPutConverter</span> <span class="keyword">extends</span> <span class="title">Converter</span>[<span class="type">Any</span>, <span class="type">Put</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">convert</span></span>(obj: <span class="type">Any</span>): <span class="type">Put</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> output = obj.asInstanceOf[java.util.<span class="type">ArrayList</span>[<span class="type">String</span>]].asScala.map(<span class="type">Bytes</span>.toBytes).toArray</span><br><span class="line">    <span class="keyword">val</span> put = <span class="keyword">new</span> <span class="type">Put</span>(output(<span class="number">0</span>))</span><br><span class="line">    put.add(output(<span class="number">1</span>), output(<span class="number">2</span>), output(<span class="number">3</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>StringListToPutConverter</code> 的工作原理是，将传入的元素强制类型转换为 <code>java.util.ArrayList[String]</code>，将第一个元素作为行键、第二个元素作为列族名、第三个元素作为列名、第四个元素作为值，四个值都转换为 <code>byte[]</code> 后上传至 HBase。</p>
<p>所以我们可以修改这个类，实现存入类型的多样化。</p>
<p>举个例子，如果我想存入一个 <code>java.math.BigDecimal</code>，那实现的方法就是：在 pyspark 程序中，将数字转换成 <code>str</code> 类型，调用我们自己写的一个 converter：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.math.<span class="type">BigDecimal</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StringListToBigDecimalToPutConverter</span> <span class="keyword">extends</span> <span class="title">Converter</span>[<span class="type">Any</span>, <span class="type">Put</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">convert</span></span>(obj: <span class="type">Any</span>): <span class="type">Put</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> output = obj.asInstanceOf[java.util.<span class="type">ArrayList</span>[<span class="type">String</span>]].asScala.toArray</span><br><span class="line">    <span class="keyword">val</span> put = <span class="keyword">new</span> <span class="type">Put</span>(<span class="type">Bytes</span>.toBytes(output(<span class="number">0</span>)))</span><br><span class="line">    put.add(</span><br><span class="line">        <span class="type">Bytes</span>.toBytes(output(<span class="number">1</span>)),</span><br><span class="line">        <span class="type">Bytes</span>.toBytes(output(<span class="number">2</span>)),</span><br><span class="line">        <span class="type">Bytes</span>.toBytes(<span class="keyword">new</span> <span class="type">BigDecimal</span>(output(<span class="number">3</span>)))</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>就可以实现存入的值是 <code>java.math.BigDecimal</code> 了。</p>
<h1 id="CDH-5-9-以前的版本，python3，master-选定为-yarn-时的-bug"><a href="#CDH-5-9-以前的版本，python3，master-选定为-yarn-时的-bug" class="headerlink" title="CDH 5.9 以前的版本，python3，master 选定为 yarn 时的 bug"></a>CDH 5.9 以前的版本，python3，master 选定为 yarn 时的 bug</h1><p>CDH 5.9 以前的版本在使用 yarn 作为 spark master 的时候，如果使用 python3，会出现 yarn 內部 <code>topology.py</code> 这个文件引发的 bug。这个文件是 python2 的语法，我们使用 python3 运行任务的时候，python3 的解释器在处理这个文件时会出错。</p>
<p>解决方案是：将这个文件重写为 python3 的版本，每次在重启 yarn 之后，将这个文件复制到所有机器的 <code>/etc/hadoop/conf.cloudera.yarn/</code>目录下。</p>
<p>以下是 python3 版本的 <code>topology.py</code>。</p>
<p><code>topology.py</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Copyright (c) 2010-2012 Cloudera, Inc. All rights reserved.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">This script is provided by CMF for hadoop to determine network/rack topology.</span></span><br><span class="line"><span class="string">It is automatically generated and could be replaced at any time. Any changes</span></span><br><span class="line"><span class="string">made to it will be lost when this happens.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> xml.dom.minidom</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">  MAP_FILE = <span class="string">'&#123;&#123;CMF_CONF_DIR&#125;&#125;/topology.map'</span></span><br><span class="line">  DEFAULT_RACK = <span class="string">'/default'</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> <span class="string">'CMF_CONF_DIR'</span> <span class="keyword">in</span> MAP_FILE:</span><br><span class="line">    <span class="comment"># variable was not substituted. Use this file's dir</span></span><br><span class="line">    MAP_FILE = os.path.join(os.path.dirname(__file__), <span class="string">"topology.map"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># We try to keep the default rack to have the same</span></span><br><span class="line">  <span class="comment"># number of elements as the other hosts available.</span></span><br><span class="line">  <span class="comment"># There are bugs in some versions of Hadoop which</span></span><br><span class="line">  <span class="comment"># make the system error out.</span></span><br><span class="line">  max_elements = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">  map = dict()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    mapFile = open(MAP_FILE, <span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">    dom = xml.dom.minidom.parse(mapFile)</span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> dom.getElementsByTagName(<span class="string">"node"</span>):</span><br><span class="line">      rack = node.getAttribute(<span class="string">"rack"</span>)</span><br><span class="line">      max_elements = max(max_elements, rack.count(<span class="string">"/"</span>))</span><br><span class="line">      map[node.getAttribute(<span class="string">"name"</span>)] = node.getAttribute(<span class="string">"rack"</span>)</span><br><span class="line">  <span class="keyword">except</span>:</span><br><span class="line">    default_rack = <span class="string">""</span>.join([ DEFAULT_RACK <span class="keyword">for</span> _ <span class="keyword">in</span> range(max_elements)])</span><br><span class="line">    print(default_rack)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">  default_rack = <span class="string">""</span>.join([ DEFAULT_RACK <span class="keyword">for</span> _ <span class="keyword">in</span> range(max_elements)])</span><br><span class="line">  <span class="keyword">if</span> len(sys.argv)==<span class="number">1</span>:</span><br><span class="line">    print(default_rack)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">" "</span>.join([map.get(i, default_rack) <span class="keyword">for</span> i <span class="keyword">in</span> sys.argv[<span class="number">1</span>:]]))</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">  sys.exit(main())</span><br></pre></td></tr></table></figure></p>
</div><div class="article-tags size-small is-uppercase mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/blog/tags/software/">software</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/spark/">Spark</a></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button is-info donate"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/blog/img/alipay.jpeg" alt="Alipay"></span></a><a class="button is-success donate"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/blog/img/wechat.jpeg" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/blog/2019/04/19/ubuntu-boot-%E6%BB%A1%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">ubuntu /boot 满了怎么办</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/blog/2019/04/07/mxnet-optimizing-memory-consumption-in-deep-learning/"><span class="level-item">MXNet: Optimizing Memory Consumption in Deep Learning</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="is-rounded" src="/blog/img/avatar.png" alt="Davidham"></figure><p class="title is-size-4 is-block line-height-inherit">Davidham</p><p class="is-size-6 is-block">阿里菜鸟-时空数据挖掘-算法工程师</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Hangzhou</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/blog/archives"><p class="title">85</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/blog/categories"><p class="title">8</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/blog/tags"><p class="title">37</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/davidham3" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/davidham3"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="mail" href="mailto:mengxian.sc@alibaba-inc.com"><i class="fa fa-envelope"></i></a></div></div></div><!--!--><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/blog/categories/algorithms/"><span class="level-start"><span class="level-item">algorithms</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/dataset/"><span class="level-start"><span class="level-item">dataset</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/software/"><span class="level-start"><span class="level-item">software</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B3%E5%8F%B0/"><span class="level-start"><span class="level-item">分布式平台</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">机器学习</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"><span class="level-start"><span class="level-item">自然语言处理</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文阅读笔记</span></span><span class="level-end"><span class="level-item tag">50</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E9%9A%8F%E7%AC%94/"><span class="level-start"><span class="level-item">随笔</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/blog/tags/attention/"><span class="tag">Attention</span><span class="tag is-grey-lightest">5</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/graph/"><span class="tag">Graph</span><span class="tag is-grey-lightest">27</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/hadoop/"><span class="tag">Hadoop</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/kafka/"><span class="tag">Kafka</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/ner/"><span class="tag">NER</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/resnet/"><span class="tag">ResNet</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/sequence/"><span class="tag">Sequence</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/spark/"><span class="tag">Spark</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/spatial-temporal/"><span class="tag">Spatial-temporal</span><span class="tag is-grey-lightest">15</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/time-series/"><span class="tag">Time Series</span><span class="tag is-grey-lightest">12</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/algorithms/"><span class="tag">algorithms</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/computer-vision/"><span class="tag">computer vision</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/dataset/"><span class="tag">dataset</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/deep-learning/"><span class="tag">deep learning</span><span class="tag is-grey-lightest">48</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/event-sequence/"><span class="tag">event sequence</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/graph-convolutional-network/"><span class="tag">graph convolutional network</span><span class="tag is-grey-lightest">20</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/image-style-transfer/"><span class="tag">image style transfer</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/implicit-feedback/"><span class="tag">implicit feedback</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/language-modeling/"><span class="tag">language modeling</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/large-scale-learning/"><span class="tag">large-scale learning</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/learning-representations/"><span class="tag">learning representations</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/machine-learning/"><span class="tag">machine learning</span><span class="tag is-grey-lightest">19</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/machine-translation/"><span class="tag">machine translation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/natural-language-processing/"><span class="tag">natural language processing</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/normalization/"><span class="tag">normalization</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/point-process/"><span class="tag">point process</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/recommender-system/"><span class="tag">recommender system</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/reinforcement-learning/"><span class="tag">reinforcement learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/seq2seq/"><span class="tag">seq2seq</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/software/"><span class="tag">software</span><span class="tag is-grey-lightest">16</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/softwares/"><span class="tag">softwares</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/super-resolution/"><span class="tag">super resolution</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/time-series/"><span class="tag">time series</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/virtual-machine/"><span class="tag">virtual machine</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/vscode/"><span class="tag">vscode</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E5%B7%B2%E5%A4%8D%E7%8E%B0/"><span class="tag">已复现</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E9%9A%8F%E7%AC%94/"><span class="tag">随笔</span><span class="tag is-grey-lightest">1</span></a></div></div></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><h3 class="menu-label">Recent</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2022-05-20T15:35:04.000Z">2022-05-20</time></p><p class="title is-6"><a class="link-muted" href="/blog/2022/05/20/individual-mobility-prediction-via-attentive-marked-temporal-point-processes/">Individual Mobility Prediction via Attentive Marked Temporal Point Processes</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2022-05-19T14:54:43.000Z">2022-05-19</time></p><p class="title is-6"><a class="link-muted" href="/blog/2022/05/19/unsupervised-scalable-representation-learning-for-multivariate-time-series/">Unsupervised Scalable Representation Learning for Multivariate Time Series</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2022-05-17T13:11:14.000Z">2022-05-17</time></p><p class="title is-6"><a class="link-muted" href="/blog/2022/05/17/fully-neural-network-based-model-for-general-temporal-point-processes/">Fully Neural Network based Model for General Temporal Point Processes</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2022-05-01T09:18:43.000Z">2022-05-01</time></p><p class="title is-6"><a class="link-muted" href="/blog/2022/05/01/recurrent-marked-temporal-point-processes-embedding-event-history-to-vector/">Recurrent Marked Temporal Point Processes: Embedding Event History to Vector</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2022-04-27T15:37:58.000Z">2022-04-27</time></p><p class="title is-6"><a class="link-muted" href="/blog/2022/04/27/time-dependent-representation-for-neural-event-sequence-prediction/">Time-dependent representation for neural event sequence prediction</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/blog/archives/2022/05/"><span class="level-start"><span class="level-item">May 2022</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2020/05/"><span class="level-start"><span class="level-item">May 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2020/02/"><span class="level-start"><span class="level-item">February 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2020/01/"><span class="level-start"><span class="level-item">January 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/12/"><span class="level-start"><span class="level-item">December 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/11/"><span class="level-start"><span class="level-item">November 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/08/"><span class="level-start"><span class="level-item">August 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/07/"><span class="level-start"><span class="level-item">July 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/06/"><span class="level-start"><span class="level-item">June 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/05/"><span class="level-start"><span class="level-item">May 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/04/"><span class="level-start"><span class="level-item">April 2019</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/03/"><span class="level-start"><span class="level-item">March 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/02/"><span class="level-start"><span class="level-item">February 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/01/"><span class="level-start"><span class="level-item">January 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/12/"><span class="level-start"><span class="level-item">December 2018</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/10/"><span class="level-start"><span class="level-item">October 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/09/"><span class="level-start"><span class="level-item">September 2018</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/08/"><span class="level-start"><span class="level-item">August 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/07/"><span class="level-start"><span class="level-item">July 2018</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/06/"><span class="level-start"><span class="level-item">June 2018</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/05/"><span class="level-start"><span class="level-item">May 2018</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/04/"><span class="level-start"><span class="level-item">April 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/03/"><span class="level-start"><span class="level-item">March 2018</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/02/"><span class="level-start"><span class="level-item">February 2018</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/01/"><span class="level-start"><span class="level-item">January 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2017/08/"><span class="level-start"><span class="level-item">August 2017</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/"><img src="/blog/img/logo.svg" alt="Davidham&#039;s blog" height="28"></a><p class="size-small"><span>&copy; 2022 Davidham</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://davidham3.github.io',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/blog/js/back_to_top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>