<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Self-Attention Graph Pooling - Davidham&#039;s blog</title><meta description="ICML 2019，原文地址：Self-Attention Graph Pooling"><meta property="og:type" content="blog"><meta property="og:title" content="Self-Attention Graph Pooling"><meta property="og:url" content="https://davidham3.github.io/2019/06/25/self-attention-graph-pooling/"><meta property="og:site_name" content="Davidham&#039;s blog"><meta property="og:description" content="ICML 2019，原文地址：Self-Attention Graph Pooling"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://davidham3.github.io/blog/2019/06/25/self-attention-graph-pooling/Fig1.JPG"><meta property="og:image" content="https://davidham3.github.io/blog/2019/06/25/self-attention-graph-pooling/Fig2.JPG"><meta property="og:image" content="https://davidham3.github.io/blog/2019/06/25/self-attention-graph-pooling/Table1.JPG"><meta property="og:image" content="https://davidham3.github.io/blog/2019/06/25/self-attention-graph-pooling/Table2.JPG"><meta property="og:image" content="https://davidham3.github.io/blog/2019/06/25/self-attention-graph-pooling/Table3.JPG"><meta property="og:image" content="https://davidham3.github.io/blog/2019/06/25/self-attention-graph-pooling/Table4.JPG"><meta property="og:image" content="https://davidham3.github.io/blog/2019/06/25/self-attention-graph-pooling/Fig3.JPG"><meta property="article:published_time" content="2019-06-25T08:34:02.000Z"><meta property="article:modified_time" content="2022-04-25T14:58:30.820Z"><meta property="article:author" content="Davidham"><meta property="article:tag" content="Graph"><meta property="article:tag" content="deep learning"><meta property="article:tag" content="graph convolutional network"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/blog/2019/06/25/self-attention-graph-pooling/Fig1.JPG"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://davidham3.github.io/2019/06/25/self-attention-graph-pooling/"},"headline":"Davidham's blog","image":[],"datePublished":"2019-06-25T08:34:02.000Z","dateModified":"2022-04-25T14:58:30.820Z","author":{"@type":"Person","name":"Davidham"},"description":"ICML 2019，原文地址：Self-Attention Graph Pooling"}</script><link rel="canonical" href="https://davidham3.github.io/2019/06/25/self-attention-graph-pooling/"><link rel="icon" href="/blog/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/"><img src="/blog/img/logo.svg" alt="Davidham&#039;s blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/blog/">Home</a><a class="navbar-item" href="/blog/archives">Archives</a><a class="navbar-item" href="/blog/categories">Categories</a><a class="navbar-item" href="/blog/tags">Tags</a><a class="navbar-item" href="/blog/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2019-06-25T08:34:02.000Z" title="2019-06-25T08:34:02.000Z">2019-06-25</time><span class="level-item"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></span><span class="level-item">22 minutes read (About 3279 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Self-Attention Graph Pooling</h1><div class="content"><p>ICML 2019，原文地址：<a href="https://arxiv.org/abs/1904.08082">Self-Attention Graph Pooling</a><br><a id="more"></a></p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>这些年有一些先进的方法将深度学习应用到了图数据上。研究专注于将卷积神经网络推广到图数据上，包括重新定义图上的卷积和下采样（池化）。推广的卷积方法已经被证明有性能提升且被广泛使用。但是，下采样的方法仍然是一个难题且有提升空间。我们提出了一个基于自注意力的图的池化方法。使用图卷积的自注意力使得我们的池化方法可以同时考虑顶点特征和图的拓扑结构。为了确保一个公平的对比，我们使用了相同的训练步骤和模型架构。实验结果显示我们的方法有更高的分类精度。</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>CNN 成功利用了图像、语音、视频数据中的欧氏空间（网格结构）。CNN 由卷积层和下采样层（池化层）组成。卷积层和池化层挖掘了网格数据的平移不变性和compositionality（这个我不知道是什么。。。）。结果是，CNN 用少量的参数就可以表现的很好。</p>
<p>然而，很多数据是非欧空间上的。社交网络、生物蛋白质网络、分子网络可以表示成网络。将 CNN 应用在非欧空间上的尝试已经获得了成功。很多研究重新定义了图上的卷积和池化。</p>
<p>对于图卷积的池化操作现在比较少。之前图的池化的研究只考虑图的拓扑结构 (Defferrard et al., 2016; Rhee et al., 2018)。一些方法利用了结点的特征获得一个小图的表示。最近，Ying et al.; Gao &amp; Ji; Cangea et al. 提出了创新的池化方法，可以层级的表示图。这些方法使得图神经网络可以通过端到端的形式，在池化后获得尺寸缩减的图。</p>
<p>然而，上述的池化方法仍有提升空间。举个例子，Ying et al. 的可微层级池化方法有平方级别的空间复杂度，参数依赖于顶点数。Gao &amp; Ji; Cangea et al. 解决了复杂度的问题，但是没有考虑图的拓扑结构。</p>
<p>我们提出的 SAGPool 是一个层次的自注意力图池化方法。我们的方法可以通过端到端的方式使用相对较少的参数学习到层次表示。自注意力机制用来区分结点是否丢弃掉还是保留。由于自注意力机制使用图卷积计算注意力分数，结点特征和图的拓扑结构可以被考虑其中。一句话，SAGPool 有前面方法的优点，是第一个使用自注意力用于池化的方法，并且获得了很好的性能。代码已经在 Github 上开源了。</p>
<h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h1><h2 id="2-1-Graph-Convolution"><a href="#2-1-Graph-Convolution" class="headerlink" title="2.1. Graph Convolution"></a>2.1. Graph Convolution</h2><p>图上的卷积要么是基于谱的，要么是非谱的。谱方法专注于在傅里叶域上定义卷积，利用使用图拉普拉斯矩阵的谱滤波器。Kipf &amp; Welling 提出了一个层级传播的规则，简化了使用切比雪夫展开来趋近拉普拉斯矩阵的方法。非谱方法的目标是定义一个卷积操作，可以直接应用在图上。通常来说，非谱方法，中心结点在特征传入下层之前聚合邻接结点的特征。Hamilton et al. 提出了 GraphSAGE，通过采样和聚合学习结点的嵌入。尽管 GraphSAGE 会采样固定数量的邻居，GAT 基于注意力机制，在所有的邻居上计算结点表示。两个方法在图相关的任务上都有提升。</p>
<h2 id="2-2-Graph-Pooling"><a href="#2-2-Graph-Pooling" class="headerlink" title="2.2. Graph Pooling"></a>2.2. Graph Pooling</h2><p>池化层通过缩减表示的大小，使得 CNN 能减少参数的数量，因此能避免过拟合。为了泛化 CNN，GNN 上的池化是必要的。图的池化方法可以归入三类：基于拓扑的，基于全局的，基于层次的。</p>
<p><strong>Topology based pooling</strong> 早期的工作使用图的缩减算法，而不是神经网络。谱聚类算法使用特征值分解获得缩减的图。然而，特征值分解的时间复杂度高。Graclus (Dhillon et al., 2007) 不使用特征向量计算给定图的聚类结果，而是通过一个谱聚类的目标函数与一个带权的核 k-means 目标函数的等价性。即便在最近的 GNN 模型中，Graclus 也被使用作为一个池化单元。</p>
<p><strong>Global pooling</strong> 不像之前的方法，全局池化方法考虑图的特征。全局池化方法在每层聚合表示的时候使用加和的方式而不是用神经网络。这个方法可以处理不同结构的图，因为它获得了所有的表示。Gilmer et al. 将 GNN 看作是一种信息的传递规则，提出了一个通用框架用于图分类，整个图的表示可以通过使用 Set2Set (Vinyals et al., 2015) 来获得。SortPool (Zhang et al., 2018b) 根据一个图的结构角色对结点嵌入排序，将排序后的表示传入下一层。</p>
<p><strong>Hierarchical pooling</strong> 全局池化方法不学习层次表示，但是对于捕获图的结构信息来说，层次表示很关键。层次池化的动机在于在每层构建一个模型，这个模型可以学习基于特征的或基于拓扑的顶点分配。Ying et al. 提出了 DiffPool，这是一种可微的图的池化方法，可以以端到端的形式学习分配矩阵。在层 $l$ 学习到的分配矩阵 $S^{(l)} \in \mathbb{R}^{n_l \times n_{l+1}}$ 包含了层 $l$ 中的结点在 $l + 1$ 层被分配到类簇的概率。$n_l$ 表示层 $l$ 的结点数。结点通过下式来分配：</p>
<script type="math/tex; mode=display">\tag{1}
S^{(l)} = \text{softmax}(\text{GNN}_l (A^{(l)}, X^{(l)})) \\
A^{(l+1)} = S^{(l)\text{T}} A^{(l)} S^{(l)}</script><p>$X$ 表示矩阵的结点特征，$A$ 是邻接矩阵。</p>
<p>Cangea et al. 使用 gPool (Gao &amp; Ji, 2019) 获得了和 DiffPool 相当的性能。gPool 需要 $\mathcal{O}(\vert V \vert + \vert E \vert)$ 的空间复杂度，DiffPool 需要 $\mathcal{O}(k \vert V \vert^2)$ 的空间复杂度。$V$，<br>$E$，$k$ 分别表示顶点、边、池化比例。gPool 使用一个可学习的向量 $p$ 计算投影分数，然后使用这个分数选择最高的结点。投影分数通过 $p$ 和所有结点的特征向量的内积获得。分数表示结点可以获得的信息量。下面的式子大体的描述了 gPool 中的池化步骤：</p>
<script type="math/tex; mode=display">\tag{2}
y = X^{(l)} \mathbf{p}^{(l)} / \Vert \mathbf{p}^{(l)} \Vert, \text{idx=top-rank}(y, \lceil kN \rceil) \\
A^{(l+1)} = A^{(l)}_{\text{idx,idx}}</script><p>如式 2，图的拓扑结构不影响投影分数。</p>
<p>为了进一步提高图的池化，我们提出了 SAGPool，可以在可观的时间和空间复杂度上利用特征和拓扑结构生成层次表示。</p>
<h1 id="3-Proposed-Method"><a href="#3-Proposed-Method" class="headerlink" title="3. Proposed Method"></a>3. Proposed Method</h1><p>SAGPool 的关键是它使用了 GNN 得到的注意力分数。SAGPool 层和模型架构分别是图 1 和图 2.</p>
<center><img src="/blog/2019/06/25/self-attention-graph-pooling/Fig1.JPG" class title="Figure1"></center>

<center><img src="/blog/2019/06/25/self-attention-graph-pooling/Fig2.JPG" class title="Figure2"></center>

<h2 id="3-1-Self-Attention-Graph-Pooling"><a href="#3-1-Self-Attention-Graph-Pooling" class="headerlink" title="3.1 Self-Attention Graph Pooling"></a>3.1 Self-Attention Graph Pooling</h2><p><strong>Self-attention mask</strong> 注意力机制广泛应用在最近的深度学习研究中。这样的机制使得模型可以更专注于重要的特征，不那么关注不重要的特征。自注意力一般称为内注意力，允许输入特征作为自身注意力的标准 (Vaswani et al., 2017)。我们使用图卷积获得自注意力分数。举个例子，如果图卷积的公式是 Kipf &amp; Welling 使用的，那么自注意力分数 $Z \in \mathbb{R}^{N \times 1}$ 通过下式计算：</p>
<script type="math/tex; mode=display">\tag{3}
Z = \sigma(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} X \Theta_{att})</script><p>$\sigma$ 是激活函数，如 $tanh$，$\tilde{A} \in \mathbb{R}^{N \times N}$ 是有自连接的邻接矩阵，$\tilde{D} \in \mathbb{R}^{N \times N}$ 是度矩阵，$X \in \mathbb{R}^{N \times F}$ 是图的特征矩阵，$\Theta_{att} \in \mathbb{R}^{F \times 1}$ 是 SAGPool 层仅有的参数。通过利用图卷积获得自注意力分数，池化的结果是同时基于图的特征和拓扑结构的。我们利用 Gao &amp; Ji; Cangea et al. 的结点选择方法，保留了输入的图的一部分结点，甚至当图的尺寸和结构改变时。池化比例 $k \in (0, 1]$ 是一个超参数决定了保留多少结点。基于 $Z$ 的值选择最高的 $\lceil kN \rceil$ 个结点。</p>
<script type="math/tex; mode=display">\tag{4}
\text{idx = top-rank}(Z, \lceil kN \rceil), Z_{mask} = Z_{\text{idx}}</script><p>$\text{top-rank}$ 返回最高的 $\lceil kN \rceil$ 个值的下标，$\cdot_{\text{idx}}$ 是下标操作，$Z_{mask}$ 是特征的注意力 mask。</p>
<p><strong>Graph pooling</strong> 输入的图通过图 1 中的 <strong>masking</strong> 操作。</p>
<script type="math/tex; mode=display">\tag{5}
X' = X_{idx,:}, X_{out} = X' \odot Z_{mask}, A_{out} = A_{\text{idx, idx}}</script><p>其中 $X_{\text{idx,:}}$ 是指定行下标的特征矩阵，每行表示一个结点，$\odot$ 是 elementwise 乘积，$A_{\text{idx, idx}}$ 是指定行下标和列下标的邻接矩阵。$X_{out}$ 和 $A_{out}$ 是新的特征矩阵和对应的邻接矩阵。</p>
<p><strong>Variation of SAGPool</strong> 使用图卷积的主要原因是为了反映图的特征和拓扑结构。可以使用不同的图卷积来替换式 3 中的图卷积。计算注意力机制 $Z \in \mathbb{R}^{N \times 1}$ 的泛化公式如下：</p>
<script type="math/tex; mode=display">\tag{6}
Z = \sigma(\text{GNN}(X, A))</script><p>$X$ 和 $A$ 是特征矩阵和邻接矩阵。</p>
<p>除了使用邻接结点还可以使用多跳结点来计算注意力分数。式 7 和式 8 分别使用了两跳连接和堆叠 GNN 层。增加邻接矩阵的平方增加了两条邻居：</p>
<script type="math/tex; mode=display">\tag{7}
Z = \sigma(\text{GNN}(X, A + A^2))</script><p>堆叠 GNN 层可以间接的聚合两跳结点。这样的话，非线性层和参数的数量就增加了：</p>
<script type="math/tex; mode=display">\tag{8}
Z = \sigma(\text{GNN}_2 (\sigma(\text{GNN}_1 (X, A)), A))</script><p>式 7 和式 8 可以利用多跳连接。</p>
<p>另一个变体是平均多个注意力分数。平均注意力分数通过 $M$ 个 GNN 获得：</p>
<script type="math/tex; mode=display">\tag{9}
Z = \frac{1}{m} \sum_m \sigma(\text{GNN}_m (X, A))</script><p>在论文中，式 7，8，9 的模型分别记为 $\rm {SAGPool}_{augmentation}$，$\rm {SAGPool}_{serial}$，$\rm {SAGPool}_{parallel}$。</p>
<h2 id="3-2-Model-Architecture"><a href="#3-2-Model-Architecture" class="headerlink" title="3.2 Model Architecture"></a>3.2 Model Architecture</h2><p>根据 Lipton &amp; Steinhardt 的研究，如果对一个模型做很多修改，那很难知道是哪部分改进起的作用。为了一个公平的对比，我们使用了 Zhang et al. 和 Cangea et al. 的模型来对比我们的方法。</p>
<p><strong>Convolution layer</strong> 如 2.1 节提到的，有很多图卷积的定义。其他类型的图卷积可能也能提升性能，但是我们利用的是 Kipf &amp; Welling 提出的广泛使用的图卷积。式 10 和式3 一样，除了 $\Theta$ 的维度：</p>
<script type="math/tex; mode=display">\tag{10}
h^{(l+1)} = \sigma(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} h^{(l)} \Theta)</script><p>其中 $h^{(l)}$ 是第 $l$ 层的节点表示，$\Theta \in \mathbb{R}^{F \times F’}$ 是卷积核。使用 ReLU 作为激活函数。</p>
<p><strong>Readout layer</strong> 受 JK-net 的启发，Cangea et al. 提出了一个 readout 层，聚合结点的特征生成一个固定大小的表示。readout 层的聚合特征如下：</p>
<script type="math/tex; mode=display">\tag{11}
s = \frac{1}{N} \sum^N_{i=1} x_i \mid \mid \mathop{max}\limits^N_{i=1} x_i</script><p>$N$ 是结点数，$x_i$ 是第 $i$ 个结点的特征向量，$\mid \mid$ 表示拼接。</p>
<p><strong>Global pooling architecture</strong> 我们实现了 Zhang et al. 提出的全局池化结构。如图 2 所示，全局池化结构由三层图卷积层组成，每层的输出拼接在一起。结点特征在 readout 层聚合，然后接一个池化层。图的特征表示传入线性层用来分类。</p>
<p><strong>Hierarchical pooling architecture</strong> 在这部分设置中，我们实现了 Cangea et al. 的层次池化结构。如图 2 所示，结构包含了三个块，每个块由一个卷积层和一个池化层组成。每个块的输出通过一个 readout 层聚合。每个 readout 层的输出之和放入线性层做分类。</p>
<center><img src="/blog/2019/06/25/self-attention-graph-pooling/Table1.JPG" class title="Table1"></center>

<h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h1><p>我们在图分类上评估了全局池化和层次池化。</p>
<h2 id="4-1-Datasets"><a href="#4-1-Datasets" class="headerlink" title="4.1. Datasets"></a>4.1. Datasets</h2><p>5 个数据集。</p>
<center><img src="/blog/2019/06/25/self-attention-graph-pooling/Table2.JPG" class title="Table2"></center>

<center><img src="/blog/2019/06/25/self-attention-graph-pooling/Table3.JPG" class title="Table3"></center>

<center><img src="/blog/2019/06/25/self-attention-graph-pooling/Table4.JPG" class title="Table4"></center>

<center><img src="/blog/2019/06/25/self-attention-graph-pooling/Fig3.JPG" class title="Figure3"></center></div><div class="article-tags size-small is-uppercase mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/blog/tags/graph/">Graph</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/deep-learning/">deep learning</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/graph-convolutional-network/">graph convolutional network</a></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button is-info donate"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/blog/img/alipay.jpeg" alt="Alipay"></span></a><a class="button is-success donate"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/blog/img/wechat.jpeg" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/blog/2019/07/12/stg2seq-spatial-temporal-graph-to-sequence-model-for-multi-step-passenger-demand-forecasting/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger Demand Forecasting</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/blog/2019/05/29/session-based-social-recommendation-via-dynamic-graph-attention-networks/"><span class="level-item">Session-based Social Recommendation via Dynamic Graph Attention Networks</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="is-rounded" src="/blog/img/avatar.png" alt="Davidham"></figure><p class="title is-size-4 is-block line-height-inherit">Davidham</p><p class="is-size-6 is-block">阿里菜鸟-时空数据挖掘-算法工程师</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Hangzhou</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/blog/archives"><p class="title">81</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/blog/categories"><p class="title">8</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/blog/tags"><p class="title">34</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/davidham3" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/davidham3"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="mail" href="mailto:mengxian.sc@alibaba-inc.com"><i class="fa fa-envelope"></i></a></div></div></div><!--!--><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/blog/categories/algorithms/"><span class="level-start"><span class="level-item">algorithms</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/dataset/"><span class="level-start"><span class="level-item">dataset</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/software/"><span class="level-start"><span class="level-item">software</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B3%E5%8F%B0/"><span class="level-start"><span class="level-item">分布式平台</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">机器学习</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"><span class="level-start"><span class="level-item">自然语言处理</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文阅读笔记</span></span><span class="level-end"><span class="level-item tag">46</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E9%9A%8F%E7%AC%94/"><span class="level-start"><span class="level-item">随笔</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/blog/tags/attention/"><span class="tag">Attention</span><span class="tag is-grey-lightest">5</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/graph/"><span class="tag">Graph</span><span class="tag is-grey-lightest">27</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/hadoop/"><span class="tag">Hadoop</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/kafka/"><span class="tag">Kafka</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/ner/"><span class="tag">NER</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/resnet/"><span class="tag">ResNet</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/sequence/"><span class="tag">Sequence</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/spark/"><span class="tag">Spark</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/spatial-temporal/"><span class="tag">Spatial-temporal</span><span class="tag is-grey-lightest">15</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/time-series/"><span class="tag">Time Series</span><span class="tag is-grey-lightest">12</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/algorithms/"><span class="tag">algorithms</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/computer-vision/"><span class="tag">computer vision</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/dataset/"><span class="tag">dataset</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/deep-learning/"><span class="tag">deep learning</span><span class="tag is-grey-lightest">44</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/event-sequence/"><span class="tag">event sequence</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/graph-convolutional-network/"><span class="tag">graph convolutional network</span><span class="tag is-grey-lightest">20</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/image-style-transfer/"><span class="tag">image style transfer</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/implicit-feedback/"><span class="tag">implicit feedback</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/language-modeling/"><span class="tag">language modeling</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/large-scale-learning/"><span class="tag">large-scale learning</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/machine-learning/"><span class="tag">machine learning</span><span class="tag is-grey-lightest">19</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/machine-translation/"><span class="tag">machine translation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/natural-language-processing/"><span class="tag">natural language processing</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/normalization/"><span class="tag">normalization</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/recommender-system/"><span class="tag">recommender system</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/reinforcement-learning/"><span class="tag">reinforcement learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/seq2seq/"><span class="tag">seq2seq</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/software/"><span class="tag">software</span><span class="tag is-grey-lightest">16</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/softwares/"><span class="tag">softwares</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/super-resolution/"><span class="tag">super resolution</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/virtual-machine/"><span class="tag">virtual machine</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/vscode/"><span class="tag">vscode</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E5%B7%B2%E5%A4%8D%E7%8E%B0/"><span class="tag">已复现</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E9%9A%8F%E7%AC%94/"><span class="tag">随笔</span><span class="tag is-grey-lightest">1</span></a></div></div></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><h3 class="menu-label">Recent</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2022-04-27T15:37:58.000Z">2022-04-27</time></p><p class="title is-6"><a class="link-muted" href="/blog/2022/04/27/time-dependent-representation-for-neural-event-sequence-prediction/">Time-dependent representation for neural event sequence prediction</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2022-04-26T14:38:05.000Z">2022-04-26</time></p><p class="title is-6"><a class="link-muted" href="/blog/2022/04/26/event-sequence-metric-learning/">Event sequence metric learning</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-05-14T04:00:28.000Z">2020-05-14</time></p><p class="title is-6"><a class="link-muted" href="/blog/2020/05/14/%E5%8D%9A%E5%AE%A2%E9%87%8D%E8%A3%85/">博客重装...</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/software/">software</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-05-13T09:07:14.000Z">2020-05-13</time></p><p class="title is-6"><a class="link-muted" href="/blog/2020/05/13/neural-collaborative-filtering/">Neural Collaborative Filtering</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-02-12T16:12:07.000Z">2020-02-13</time></p><p class="title is-6"><a class="link-muted" href="/blog/2020/02/13/%E8%AE%B0%E4%B8%80%E6%AC%A1pyspark%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87%EF%BC%8Cnp-frombuffer%E7%9A%84%E4%BD%BF%E7%94%A8/">记一次pyspark性能提升，np.frombuffer的使用</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B3%E5%8F%B0/">分布式平台</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/blog/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2020/05/"><span class="level-start"><span class="level-item">May 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2020/02/"><span class="level-start"><span class="level-item">February 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2020/01/"><span class="level-start"><span class="level-item">January 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/12/"><span class="level-start"><span class="level-item">December 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/11/"><span class="level-start"><span class="level-item">November 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/08/"><span class="level-start"><span class="level-item">August 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/07/"><span class="level-start"><span class="level-item">July 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/06/"><span class="level-start"><span class="level-item">June 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/05/"><span class="level-start"><span class="level-item">May 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/04/"><span class="level-start"><span class="level-item">April 2019</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/03/"><span class="level-start"><span class="level-item">March 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/02/"><span class="level-start"><span class="level-item">February 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/01/"><span class="level-start"><span class="level-item">January 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/12/"><span class="level-start"><span class="level-item">December 2018</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/10/"><span class="level-start"><span class="level-item">October 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/09/"><span class="level-start"><span class="level-item">September 2018</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/08/"><span class="level-start"><span class="level-item">August 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/07/"><span class="level-start"><span class="level-item">July 2018</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/06/"><span class="level-start"><span class="level-item">June 2018</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/05/"><span class="level-start"><span class="level-item">May 2018</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/04/"><span class="level-start"><span class="level-item">April 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/03/"><span class="level-start"><span class="level-item">March 2018</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/02/"><span class="level-start"><span class="level-item">February 2018</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/01/"><span class="level-start"><span class="level-item">January 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2017/08/"><span class="level-start"><span class="level-item">August 2017</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/"><img src="/blog/img/logo.svg" alt="Davidham&#039;s blog" height="28"></a><p class="size-small"><span>&copy; 2022 Davidham</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://davidham3.github.io',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/blog/js/back_to_top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>