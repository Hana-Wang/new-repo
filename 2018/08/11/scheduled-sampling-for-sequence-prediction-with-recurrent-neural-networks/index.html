<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks - Davidham&#039;s blog</title><meta description="NIPS 2015. 在训练seq2seq的时候，比如像机器翻译，训练的时候，每个输出y，它所依据的前一个词，都是正确的。但是在预测的时候，输出的这个词依照的上一个词，是模型输出的词，无法保证是正确的，这就会造成模型的输入和预测的分布不一致，可能会造成错误的累积。本文提出了scheduled sampling来处理这个问题。原文链接：Scheduled Sampling for Sequence"><meta property="og:type" content="blog"><meta property="og:title" content="Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks"><meta property="og:url" content="https://davidham3.github.io/2018/08/11/scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks/"><meta property="og:site_name" content="Davidham&#039;s blog"><meta property="og:description" content="NIPS 2015. 在训练seq2seq的时候，比如像机器翻译，训练的时候，每个输出y，它所依据的前一个词，都是正确的。但是在预测的时候，输出的这个词依照的上一个词，是模型输出的词，无法保证是正确的，这就会造成模型的输入和预测的分布不一致，可能会造成错误的累积。本文提出了scheduled sampling来处理这个问题。原文链接：Scheduled Sampling for Sequence"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://davidham3.github.io/blog/2018/08/11/scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks/Fig1.jpg"><meta property="og:image" content="https://davidham3.github.io/blog/2018/08/11/scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks/Fig2.jpg"><meta property="article:published_time" content="2018-08-11T02:14:13.000Z"><meta property="article:modified_time" content="2022-04-25T14:58:30.803Z"><meta property="article:author" content="Davidham"><meta property="article:tag" content="deep learning"><meta property="article:tag" content="Sequence"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/blog/2018/08/11/scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks/Fig1.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://davidham3.github.io/2018/08/11/scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks/"},"headline":"Davidham's blog","image":["https://davidham3.github.io/blog/2018/08/11/scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks/Fig1.jpg","https://davidham3.github.io/blog/2018/08/11/scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks/Fig2.jpg"],"datePublished":"2018-08-11T02:14:13.000Z","dateModified":"2022-04-25T14:58:30.803Z","author":{"@type":"Person","name":"Davidham"},"description":"NIPS 2015. 在训练seq2seq的时候，比如像机器翻译，训练的时候，每个输出y，它所依据的前一个词，都是正确的。但是在预测的时候，输出的这个词依照的上一个词，是模型输出的词，无法保证是正确的，这就会造成模型的输入和预测的分布不一致，可能会造成错误的累积。本文提出了scheduled sampling来处理这个问题。原文链接：Scheduled Sampling for Sequence"}</script><link rel="canonical" href="https://davidham3.github.io/2018/08/11/scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks/"><link rel="icon" href="/blog/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/"><img src="/blog/img/logo.svg" alt="Davidham&#039;s blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/blog/">Home</a><a class="navbar-item" href="/blog/archives">Archives</a><a class="navbar-item" href="/blog/categories">Categories</a><a class="navbar-item" href="/blog/tags">Tags</a><a class="navbar-item" href="/blog/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2018-08-11T02:14:13.000Z" title="2018-08-11T02:14:13.000Z">2018-08-11</time><span class="level-item"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></span><span class="level-item">20 minutes read (About 3040 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks</h1><div class="content"><p>NIPS 2015. 在训练seq2seq的时候，比如像机器翻译，训练的时候，每个输出y，它所依据的前一个词，都是正确的。但是在预测的时候，输出的这个词依照的上一个词，是模型输出的词，无法保证是正确的，这就会造成模型的输入和预测的分布不一致，可能会造成错误的累积。本文提出了scheduled sampling来处理这个问题。原文链接：<a href="https://arxiv.org/abs/1506.03099">Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks</a></p>
<a id="more"></a>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>循环神经网络可以训练成给定一些输入，输出一些token的模型，比如最近的机器翻译和图像描述。现在训练这些的方法包括给定当前状态和之前的token，最大化序列中每个token的似然。在推理阶段，未知的token会被模型生成的token替代。这种在训练和推断之间的差异，会在生成序列时产生快速积累的误差。我们提出了一个递进学习策略(curriculum learning strategy)轻微地将训练过程进行一些改变，从之前的完全使用一个真实的token进行引导的策略，变成了基本使用生成的token来引导的策略。在几个序列预测的实验上表明我们的方法有很大的提升。此外，它成功的在MSCOCO图像描述2015任务上获得冠军。</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>循环神经网络可以用于处理序列，要么输入，要么输出，或者都可以。尽管他们很难在长期依赖的数据上训练，一些如LSTM的版本可以更好的适应这种问题。事实是，最近在一些序列预测问题上，包括机器翻译，contextual parsing，图像描述甚至视频描述上，这些模型表现的很好。</p>
<p>在这篇论文中，我们考虑生成可变大小的token的序列的问题，比如机器翻译，目标是给定源语言翻译成目标语言。我们也考虑当输入不是一个序列的问题，如图像描述问题，目标是对给定的图像生成一个文本描述。</p>
<p>在这两种情况，循环神经网络一般都是通过给定的输入，最大化生成目标序列的似然。实际上，这个是通过给定当前模型的状态和之前的目标token，最大化每个目标token的似然，这使得模型可以在目标token上学习一种语言模型。然而，在推断过程中，真的<em>previous</em>目标token是无法获取的，这就使得模型需要用它自己生成的token，导致模型在训练和预测时会有差异。通过使用beam search启发式的生成几个目标序列可以缓解这种差异，但是对于连续的状态空间模型，如RNN，不存在动态规划的方法，所以即便是使用beam search，考虑的序列的数量仍然会很小。</p>
<p>主要问题是，在生成序列时越早出现错误，会导致将这个错误输入进模型，然后会扩大模型的误差，因为模型会将它在训练时未见过的错误考虑到状态空间内。</p>
<p>我们提出了一个递进学习方法，在对序列预测任务上使用RNN的训练和推测时构建了桥梁。我们提出，改变训练过程，为了逐渐地使模型处理它的错误，使它在推断时也可以进行。这样，模型在训练时会探索更多的情况，因此在推断时会更鲁棒的纠正它的错误，因为它在训练时就学习过这个。我们会展示这个方法在几个序列预测问题上的结果。</p>
<h1 id="2-Proposed-Approach"><a href="#2-Proposed-Approach" class="headerlink" title="2 Proposed Approach"></a>2 Proposed Approach</h1><p>我们考虑一个监督学习任务，训练集是给定的 $N$ 个样本的输入输出对，$\lbrace X^i, Y^i \rbrace^N_{i=1}$，$X^i$ 是输入，要么静态（图像），要么动态（序列），输出 $Y^i$ 是一个可变数量的token的序列 $y^i_1, y^i_2, …, y^i_{T_i}$，token属于一个已知的词典。</p>
<h2 id="2-1-Model"><a href="#2-1-Model" class="headerlink" title="2.1 Model"></a>2.1 Model</h2><p>给定一个输入/输出对儿 $(X, Y)$，log 概率 $P(Y \mid X)$ 可由下式计算：</p>
<script type="math/tex; mode=display">\tag{1}
\begin{aligned}
\mathrm{log} P(Y \mid X) &= \mathrm{log} P(y^T_1 \mid X) \\
&= \sum^T_{t = 1} \mathrm{log} P(y_t \mid y^{t-1}_1, X)
\end{aligned}</script><p>其中，$Y$ 是长度为 $T$ 的序列，$y_1, y_2, …, y_T$。在前面的等式中，后面的项通过一个参数为 $\theta$ 的循环神经网络，通过一个状态向量 $h_t$估计得到，也就是通过前一个输出 $y_{t-1}$ 和前一个状态 $h_{t-1}$估计得到：</p>
<script type="math/tex; mode=display">\tag{2}
\mathrm{log} P(y_t \mid y^{t-1}_1, X; \theta) = \mathrm{log} P(y_t \mid h_t; \theta)</script><p>其中，$h_t$ 通过如下的一个循环神经网络计算得到：</p>
<script type="math/tex; mode=display">\tag{3}
h_t = \begin{cases}
f(X; \theta) \ \ \mathrm{if} t = 1\\
f(h_{t-1}, y_{t-1}; \theta) \mathrm{otherwise}.
\end{cases}</script><p>$P(y_t \mid h_t; \theta)$ 经常通过状态向量 $h_t$ 的一个线性变换，变换到一个 vector of scores 实现，这个向量是输出字典的每个token的分数，然后用一个 softmax 确保分数适当的归一化。$f(h, y)$通常是一个非线性函数，这个函数融合了之前的状态和之前的输出来生成当前的状态。</p>
<p>这就意味着模型专注于给定模型当前状态，学习预测下一个输出。因此，模型会以最普通的形式表示序列的概率分布——不像条件随机场以及其他的模型，在给定隐变量状态后，假设不同时间步的输出相互独立。模型的容量只会被循环层和前向传播层的表示容量限制。LSTM，因为他们能学习长范围的结构，所以对这种问题来说非常适合，也就可以学习序列上的rich distributions。</p>
<p>为了学习边长序列，一个特殊的token，<EOS>，表示序列的结束被添加进字典和模型中。在训练的过程中，<EOS>会拼接在每个序列的结尾处。在推理的时候，模型会生成tokens直到它生成了<EOS>。</EOS></EOS></EOS></p>
<h2 id="2-2-Training"><a href="#2-2-Training" class="headerlink" title="2.2 Training"></a>2.2 Training</h2><p>训练循环神经网络来解决这样的问题通常通过mini-batch随机梯度下降求解，通过给定输入数据 $X^i$，为所有的训练对儿 $(X^i, Y^i)$最大化生成正确的目标序列 $Y^i$ 的似然，找到一组参数$\theta^*$。</p>
<script type="math/tex; mode=display">\tag{4}
\theta^* = \mathop{\arg \max_\theta} \sum_{(X^i, Y^i)} \mathrm{log} P(Y^i \mid X^i; \theta)</script><h2 id="2-3-Inference"><a href="#2-3-Inference" class="headerlink" title="2.3 Inference"></a>2.3 Inference</h2><p>在推理的过程中，模型可以在给定 $X$ 的情况下通过一次生成一个token，生成整个序列 $y^T_1$。生成一个 <EOS> 后，它标志着序列的结束。对于这个过程，在时间 $t$，模型为了生成 $y_t$，需要从最后一个时间戳讲输出的token $y_{t-1}$ 作为输入。因为我们没法知道真正的上一个token是什么，我们可以要么选择模型给出的最可能的那个，要么根据这个来抽样。</EOS></p>
<p>给定 $X$ 搜索最大概率的序列 $Y$ 非常费时，因为序列的长度是组合地上升的。我们使用一个beam search来生成 $k$ 个最好的序列。我们通过维护 $m$ 个最优候选序列组成的一个堆来做这个。每次通过给每个候选序列扩充一个token并把它增加到堆内，都能得到一个新的候选序列。在这步的结尾，堆重新地剪枝到只有 $m$ 个候选序列。beam searching在没有新的序列增加的时候就会截断，然后返回 $k$ 个最优的序列。</p>
<p>尽管beam search通常用于基于HMM这样的模型的离散状态，这些模型可以使用动态规划，但是对于像RNN这样的连续状态模型就很难了，因为没有办法再连续空间内factor the followed state paths，因此在beam search解码的时候，可以控制的候选序列的实际数量是很小的。</p>
<p>在所有的情况里，如果在时间 $t-1$ 有一个错误生成，那么模型就会在一个和训练分布不同的状态空间中，而且它会在这个空间中不知所措。更糟的是，这会导致模型在决策的时候导致不好的决策的累计——a classic problem in sequential Gibbs sampling type appraoches to sampling, where future samples can have no influence on the past.</p>
<h2 id="2-4-Bridging-the-Gap-with-Scheduled-Sampling"><a href="#2-4-Bridging-the-Gap-with-Scheduled-Sampling" class="headerlink" title="2.4 Bridging the Gap with Scheduled Sampling"></a>2.4 Bridging the Gap with Scheduled Sampling</h2><p>在预测token $y_t$ 时，训练和推断的主要差别是我们是否使用前一个真实的token $y_{t-1}$，还是使用一个从模型得到的估计值 $\hat{y}_{t-1}$。</p>
<p>我们在这里提出了一个采样机制，会在训练的时候，随机地选择 $y_{t-1}$ 或 $\hat{y}_{t-1}$。假设我们使用mini-batch随机梯度下降，对于训练算法的第 $i$ 个mini-batch中预测 $y_t \in Y$ 的每个token，我们提出用抛硬币的方法，设使用真实token的概率为$\epsilon_i$，使用它估计的token的概率为$(1 - \epsilon_i)$。模型的估计值可以根据模型的概率分布$P(y_{t-1} \mid h_{t-1})$ 来采样获得，或是取 $\mathop{\arg \max_s} P(y_{t-1} = s \mid h_{t-1})$。这个过程由图1所示。</p>
<p><div align="center"><img src="/blog/2018/08/11/scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks/Fig1.jpg" class title="Figure1"></div></p>
<p>当 $\epsilon_i = 1$ 时，模型就像之前一样训练，但是当 $\epsilon_i = 0$ 时，模型就会和推断时一样训练。我们这里提出了一个递进学习策略，在训练的开始接断，从模型可能生成的token中进行采样，因为此时模型还没有训练好，这可能会使模型的收敛速度变慢，所以这里选择较多的真实token会帮助训练；另一方面，在训练快结束的时候，$\epsilon_i$ 应该更倾向于从模型的生成结果中采样，因为这个对应了推测的场景，这时我们会期望模型已经有足够好的能力来处理这个问题，并且采样出有效的tokens。</p>
<p>因此我们提出使用一个规则来减少 $\epsilon_i$ 来作为 $i$ 的函数，就像现在很多随机梯度下降算法那样降低学习率一样。这样的规则如图2所示：</p>
<p>· Linear decay: $\epsilon_i = \mathrm{max} (\epsilon, k - ci)$，其中 $0 \leq \epsilon &lt; 1$ 是给模型的真值最小的数量，$k$ 和 $c$ 提供了衰减的截距和斜率，这些依赖于收敛速度。</p>
<p>· Exponential decay: $\epsilon_i = k^i$，其中 $k &lt; 1$ 是一个依赖于期望收敛速度的常量。</p>
<p>· Inverse sigmoid decay: $\epsilon_i = k/(k + \mathrm{exp}(i/k))$，其中 $k \geq 1$ 依赖于期望收敛速度。</p>
<p>我们的方法命名为 <em>Scheduled Sampling</em>。需要注意的是在模型训练时从它的输出采样到前一个token $\hat{y}_{t-1}$时，我们可以在时间 $t \rightarrow T$ 内进行梯度的反向传播。在实验中我们没有尝试，会在未来的工作中尝试。</p>
<p><div align="center"><img src="/blog/2018/08/11/scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks/Fig2.jpg" class title="Figure2"></div></p>
</div><div class="article-tags size-small is-uppercase mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/blog/tags/deep-learning/">deep learning</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/sequence/">Sequence</a></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button is-info donate"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/blog/img/alipay.jpeg" alt="Alipay"></span></a><a class="button is-success donate"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/blog/img/wechat.jpeg" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/blog/2018/09/01/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%BC%95%E5%85%A5%E9%9A%8F%E6%9C%BA%E6%95%B0/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">决策树为什么要引入随机数</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/blog/2018/08/03/the-emerging-field-of-signal-processing-on-graphs/"><span class="level-item">The Emerging Field of Signal Processing on Graphs</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="is-rounded" src="/blog/img/avatar.png" alt="Davidham"></figure><p class="title is-size-4 is-block line-height-inherit">Davidham</p><p class="is-size-6 is-block">阿里菜鸟-时空数据挖掘-算法工程师</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Hangzhou</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/blog/archives"><p class="title">84</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/blog/categories"><p class="title">8</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/blog/tags"><p class="title">36</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/davidham3" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/davidham3"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="mail" href="mailto:mengxian.sc@alibaba-inc.com"><i class="fa fa-envelope"></i></a></div></div></div><!--!--><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/blog/categories/algorithms/"><span class="level-start"><span class="level-item">algorithms</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/dataset/"><span class="level-start"><span class="level-item">dataset</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/software/"><span class="level-start"><span class="level-item">software</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B3%E5%8F%B0/"><span class="level-start"><span class="level-item">分布式平台</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">机器学习</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"><span class="level-start"><span class="level-item">自然语言处理</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文阅读笔记</span></span><span class="level-end"><span class="level-item tag">49</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E9%9A%8F%E7%AC%94/"><span class="level-start"><span class="level-item">随笔</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/blog/tags/attention/"><span class="tag">Attention</span><span class="tag is-grey-lightest">5</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/graph/"><span class="tag">Graph</span><span class="tag is-grey-lightest">27</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/hadoop/"><span class="tag">Hadoop</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/kafka/"><span class="tag">Kafka</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/ner/"><span class="tag">NER</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/resnet/"><span class="tag">ResNet</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/sequence/"><span class="tag">Sequence</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/spark/"><span class="tag">Spark</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/spatial-temporal/"><span class="tag">Spatial-temporal</span><span class="tag is-grey-lightest">15</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/time-series/"><span class="tag">Time Series</span><span class="tag is-grey-lightest">12</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/algorithms/"><span class="tag">algorithms</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/computer-vision/"><span class="tag">computer vision</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/dataset/"><span class="tag">dataset</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/deep-learning/"><span class="tag">deep learning</span><span class="tag is-grey-lightest">47</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/event-sequence/"><span class="tag">event sequence</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/graph-convolutional-network/"><span class="tag">graph convolutional network</span><span class="tag is-grey-lightest">20</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/image-style-transfer/"><span class="tag">image style transfer</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/implicit-feedback/"><span class="tag">implicit feedback</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/language-modeling/"><span class="tag">language modeling</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/large-scale-learning/"><span class="tag">large-scale learning</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/learning-representations/"><span class="tag">learning representations</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/machine-learning/"><span class="tag">machine learning</span><span class="tag is-grey-lightest">19</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/machine-translation/"><span class="tag">machine translation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/natural-language-processing/"><span class="tag">natural language processing</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/normalization/"><span class="tag">normalization</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/recommender-system/"><span class="tag">recommender system</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/reinforcement-learning/"><span class="tag">reinforcement learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/seq2seq/"><span class="tag">seq2seq</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/software/"><span class="tag">software</span><span class="tag is-grey-lightest">16</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/softwares/"><span class="tag">softwares</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/super-resolution/"><span class="tag">super resolution</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/time-series/"><span class="tag">time series</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/virtual-machine/"><span class="tag">virtual machine</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/vscode/"><span class="tag">vscode</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E5%B7%B2%E5%A4%8D%E7%8E%B0/"><span class="tag">已复现</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E9%9A%8F%E7%AC%94/"><span class="tag">随笔</span><span class="tag is-grey-lightest">1</span></a></div></div></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><h3 class="menu-label">Recent</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2022-05-19T14:54:43.000Z">2022-05-19</time></p><p class="title is-6"><a class="link-muted" href="/blog/2022/05/19/unsupervised-scalable-representation-learning-for-multivariate-time-series/">Unsupervised Scalable Representation Learning for Multivariate Time Series</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2022-05-17T13:11:14.000Z">2022-05-17</time></p><p class="title is-6"><a class="link-muted" href="/blog/2022/05/17/fully-neural-network-based-model-for-general-temporal-point-processes/">Fully Neural Network based Model for General Temporal Point Processes</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2022-05-01T09:18:43.000Z">2022-05-01</time></p><p class="title is-6"><a class="link-muted" href="/blog/2022/05/01/recurrent-marked-temporal-point-processes-embedding-event-history-to-vector/">Recurrent Marked Temporal Point Processes: Embedding Event History to Vector</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2022-04-27T15:37:58.000Z">2022-04-27</time></p><p class="title is-6"><a class="link-muted" href="/blog/2022/04/27/time-dependent-representation-for-neural-event-sequence-prediction/">Time-dependent representation for neural event sequence prediction</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2022-04-26T14:38:05.000Z">2022-04-26</time></p><p class="title is-6"><a class="link-muted" href="/blog/2022/04/26/event-sequence-metric-learning/">Event sequence metric learning</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/blog/archives/2022/05/"><span class="level-start"><span class="level-item">May 2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2020/05/"><span class="level-start"><span class="level-item">May 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2020/02/"><span class="level-start"><span class="level-item">February 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2020/01/"><span class="level-start"><span class="level-item">January 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/12/"><span class="level-start"><span class="level-item">December 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/11/"><span class="level-start"><span class="level-item">November 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/08/"><span class="level-start"><span class="level-item">August 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/07/"><span class="level-start"><span class="level-item">July 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/06/"><span class="level-start"><span class="level-item">June 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/05/"><span class="level-start"><span class="level-item">May 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/04/"><span class="level-start"><span class="level-item">April 2019</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/03/"><span class="level-start"><span class="level-item">March 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/02/"><span class="level-start"><span class="level-item">February 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/01/"><span class="level-start"><span class="level-item">January 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/12/"><span class="level-start"><span class="level-item">December 2018</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/10/"><span class="level-start"><span class="level-item">October 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/09/"><span class="level-start"><span class="level-item">September 2018</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/08/"><span class="level-start"><span class="level-item">August 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/07/"><span class="level-start"><span class="level-item">July 2018</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/06/"><span class="level-start"><span class="level-item">June 2018</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/05/"><span class="level-start"><span class="level-item">May 2018</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/04/"><span class="level-start"><span class="level-item">April 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/03/"><span class="level-start"><span class="level-item">March 2018</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/02/"><span class="level-start"><span class="level-item">February 2018</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/01/"><span class="level-start"><span class="level-item">January 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2017/08/"><span class="level-start"><span class="level-item">August 2017</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/"><img src="/blog/img/logo.svg" alt="Davidham&#039;s blog" height="28"></a><p class="size-small"><span>&copy; 2022 Davidham</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://davidham3.github.io',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/blog/js/back_to_top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>