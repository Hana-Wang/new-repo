<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Gaussian Naive Bayes - Davidham&#039;s blog</title><meta description="假设连续型随机变量服从高斯分布的朴素贝叶斯。发现自己实现的版本比sklearn的精度低了20%左右……研究了一下差在了哪里。"><meta property="og:type" content="blog"><meta property="og:title" content="Gaussian Naive Bayes"><meta property="og:url" content="https://davidham3.github.io/2018/06/14/gaussian-naive-bayes/"><meta property="og:site_name" content="Davidham&#039;s blog"><meta property="og:description" content="假设连续型随机变量服从高斯分布的朴素贝叶斯。发现自己实现的版本比sklearn的精度低了20%左右……研究了一下差在了哪里。"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://davidham3.github.io/blog/img/og_image.png"><meta property="article:published_time" content="2018-06-14T11:33:30.000Z"><meta property="article:modified_time" content="2022-04-25T14:58:30.595Z"><meta property="article:author" content="Davidham"><meta property="article:tag" content="machine learning"><meta property="article:tag" content="已复现"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/blog/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://davidham3.github.io/2018/06/14/gaussian-naive-bayes/"},"headline":"Davidham's blog","image":["https://davidham3.github.io/blog/img/og_image.png"],"datePublished":"2018-06-14T11:33:30.000Z","dateModified":"2022-04-25T14:58:30.595Z","author":{"@type":"Person","name":"Davidham"},"description":"假设连续型随机变量服从高斯分布的朴素贝叶斯。发现自己实现的版本比sklearn的精度低了20%左右……研究了一下差在了哪里。"}</script><link rel="canonical" href="https://davidham3.github.io/2018/06/14/gaussian-naive-bayes/"><link rel="icon" href="/blog/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/"><img src="/blog/img/logo.svg" alt="Davidham&#039;s blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/blog/">Home</a><a class="navbar-item" href="/blog/archives">Archives</a><a class="navbar-item" href="/blog/categories">Categories</a><a class="navbar-item" href="/blog/tags">Tags</a><a class="navbar-item" href="/blog/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2018-06-14T11:33:30.000Z" title="2018-06-14T11:33:30.000Z">2018-06-14</time><span class="level-item"><a class="link-muted" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="level-item">16 minutes read (About 2327 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Gaussian Naive Bayes</h1><div class="content"><p>假设连续型随机变量服从高斯分布的朴素贝叶斯。发现自己实现的版本比sklearn的精度低了20%左右……研究了一下差在了哪里。<br><a id="more"></a></p>
<h1 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h1><p>朴素贝叶斯是基于贝叶斯定理与特征条件独立假设的分类器。</p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>朴素贝叶斯通过给定训练集</p>
<script type="math/tex; mode=display">T = \lbrace (x_1, y_1), (x_2, y_2), ···, (x_N, y_N)\rbrace</script><p>训练学习到联合概率分布$P(X, Y)$，通过先验概率分布</p>
<script type="math/tex; mode=display">P(Y = c_k), k = 1,2,...,K</script><p>和条件概率分布</p>
<script type="math/tex; mode=display">P(X = x \mid Y = c_k) = P(X^{(1)} = x^{(1)}, ···, X^{(n)} = x^{(n)} \mid Y = c_k), k=1,2,...,K</script><p>学习到联合概率分布$P(X, Y)$</p>
<p>由特征相互独立假设，可得</p>
<script type="math/tex; mode=display">P(X = x \mid Y = c_k) = \prod^n_{j=1}P(X^{(j)}=x^{(j)} \mid Y = c_k)</script><p>分类时，对给定的输入$x$，模型计算$P(Y = c_k \mid X = x)$，将后验概率最大的类作为$x$的类输出，后验概率计算如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    P(Y = c_k \mid X = x) &= \frac{P(X = x \mid Y = c_k)P(Y = c_k)}{\sum_kP(X = x \mid Y = c_k)P(Y = c_k)} \\
    & = \frac{P(Y = c_k) \prod_j P(X^{(j)} = x^{(j)} \mid Y = c_k)}{\sum_k P(Y = c_k) \prod_j P(X^{(j)} = x^{(j)} \mid Y = c_k)}
\end{aligned}</script><p>由于分母对任意的$c_k$都相同，故朴素贝叶斯分类器可以表示为：</p>
<script type="math/tex; mode=display">
y = \mathop{\arg\max}_{c_k} P(Y = c_k) \prod_j P(X^{(j)} = x^{(j)} \mid Y = c_k)</script><h2 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h2><ol>
<li><p>如果特征是离散型随机变量，可以使用频率用来估计概率。</p>
<script type="math/tex; mode=display">P(Y = c_k) = \frac{\sum^N_{i=1}I(y_i = c_k)}{N}, k=1,2,...,K</script><p> 设第$j$个特征的取值的集合为${a_{j1}, a_{j2}, …, a_{js_j}}$，则</p>
<script type="math/tex; mode=display">
 \begin{gathered}P(X^{(j)} = a_{jl} \mid Y = c_k) = \frac{\sum^N_{i=1}I(x^{(j)}_i = a_{jl}, y_i = c_k)}{\sum^N_{i=1}I(y_i = c_k)}\\
 j=1,2,...,n; \ l=1,2,...,S_j; \ k=1,2,...,K
 \end{gathered}</script></li>
<li><p>如果特征是连续型随机变量，可以假设正态分布来估计条件概率。</p>
<script type="math/tex; mode=display">P(X^{(j)} = a_{jl} \mid Y = c_k) = \frac{1}{\sqrt{2 \pi \sigma^2_{c_k,j}}}\exp{(- \frac{(a_{jl} - \mu_{c_k,j})^2}{2 \sigma^2_{c_k,j}})}</script><p> 这里$\mu_{c_k,j}$和$\sigma^2_{c_k,j}$分别为$Y = c_k$时，第$j$个特征的均值和方差。</p>
</li>
</ol>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>因为二值分类和$n$值分类是一样的，故以下代码只实现了$n$值分类的朴素贝叶斯分类器。<br>仓库:<a href="https://github.com/Davidham3/naive_bayes">https://github.com/Davidham3/naive_bayes</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readDataSet</span><span class="params">(filename, frequency = <span class="number">0</span>, training_set_ratio = <span class="number">0.7</span>, shuffle = True)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    read the dataset file, and shuffle, remove all punctuations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">        filename: str, the filename of the data</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        frequency: int, you will select the words that appeared more than the frequency you specified</span></span><br><span class="line"><span class="string">                    for example, if you set frequency equals 1, the program will return all words that they have </span></span><br><span class="line"><span class="string">                    appeared more than once.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        training_set_ratio: float, the ratio of training data account for in all data</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        shuffle: bool, whether to shuffle the data</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">        train_text: list, each element contains a tuple of words that in each sentence</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        train_labels: list, each element is the label of the corresponding sentence</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        test_text: list</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        test_labels: list</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        text = f.read().strip().split(<span class="string">'\n'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        np.random.shuffle(text)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">import</span> re</span><br><span class="line">    <span class="comment"># split all words by space and add them with their labels to the list "dataset"</span></span><br><span class="line">    dataset = []</span><br><span class="line">    <span class="keyword">for</span> index, i <span class="keyword">in</span> enumerate(text):</span><br><span class="line">        t = i.split(<span class="string">'\t'</span>)</span><br><span class="line">        label = t[<span class="number">0</span>]</span><br><span class="line">        t1 = re.sub(<span class="string">"[\.\!\/_,$%^*(+\"\']+|[+——！，。？?、~@#￥%……&amp;*（）]+"</span>, <span class="string">""</span>, t[<span class="number">1</span>])</span><br><span class="line">        dataset.append((label, re.split(re.compile(<span class="string">'\s+'</span>), t1)))</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"dataset's size is"</span>, len(dataset))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># split labels and words</span></span><br><span class="line">    labels, text = zip(*dataset)</span><br><span class="line">    </span><br><span class="line">    split_line = int(len(text) * training_set_ratio)</span><br><span class="line">    train_text = text[:split_line]</span><br><span class="line">    train_labels = labels[:split_line]</span><br><span class="line">    test_text = text[split_line:]</span><br><span class="line">    test_labels = labels[split_line:]</span><br><span class="line">    <span class="keyword">return</span> train_text, train_labels, test_text, test_labels</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocessing_training_data</span><span class="params">(text, labels)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    use bag of words to build features for training data</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">        text: lists, each element contains a list of words in a sentence</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        labels: lists, each element is the label of the sample corresponding to the element in text</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">        trainX: ndarray, training data, the shape of it is (number of samples, number of features)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        trainY: ndarray, labels of training data, the shape of it is (number of samples, )</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        words_table: dict, key is words, value is the index in bag of words</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        labels_table: dict, key is the label, value is the index that represents the corresponding label</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    bag_of_words = tuple(set(word <span class="keyword">for</span> words <span class="keyword">in</span> text <span class="keyword">for</span> word <span class="keyword">in</span> words))</span><br><span class="line">    words_table = &#123;i: index <span class="keyword">for</span> index, i <span class="keyword">in</span> enumerate(bag_of_words)&#125;</span><br><span class="line">    trainX = np.empty((len(text), len(bag_of_words)))</span><br><span class="line">    <span class="keyword">for</span> index, words <span class="keyword">in</span> enumerate(text):</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">            trainX[index, words_table[word]] += <span class="number">1</span></span><br><span class="line">    labels_table = &#123;i: index <span class="keyword">for</span> index, i <span class="keyword">in</span> enumerate(set(labels))&#125;</span><br><span class="line">    trainY = np.array([labels_table[i] <span class="keyword">for</span> i <span class="keyword">in</span> labels])</span><br><span class="line">    <span class="keyword">return</span> trainX, trainY, words_table, labels_table</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocessing_testing_data</span><span class="params">(text, labels, words_table, labels_table)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    use bag of words to build features for testing data</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">        text: lists, each element contains a list of words in a sentence</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        labels: lists, each element is the label of the sample corresponding to the element in text</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        words_table: dict, key is words, value is the index in bag of words</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        labels_table: dict, key is the label, value is the index that represents the corresponding label</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">        testX: ndarray, testing data, the shape of it is (number of samples, number of features)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        testY: ndarray, labels of testing data, the shape of it is (number of samples, )</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    testX = np.empty((len(text), len(words_table)))</span><br><span class="line">    <span class="keyword">for</span> index, words <span class="keyword">in</span> enumerate(text):</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">            col = words_table.get(word)</span><br><span class="line">            <span class="keyword">if</span> col <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                testX[index, words_table[word]] += <span class="number">1</span></span><br><span class="line">    testY = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> labels:</span><br><span class="line">        l = labels_table.get(i)</span><br><span class="line">        <span class="keyword">if</span> l <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            testY.append(l)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            labels_table[i] = len(labels_table)</span><br><span class="line">            testY.append(labels_table[i])</span><br><span class="line">    testY = np.array(testY)</span><br><span class="line">    <span class="keyword">return</span> testX, testY</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GaussianNB</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Gaussian naive bayes for continous features</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.probability_of_y = &#123;&#125;</span><br><span class="line">        self.mean = &#123;&#125;</span><br><span class="line">        self.var = &#123;&#125;</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, trainX, trainY)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        use trainX and trainY to compute the prior probability for each class</span></span><br><span class="line"><span class="string">        and then compute the mean and variance for each features for each class</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">            trainX: ndarray, training data, the shape of it is (number of samples, number of features)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">            trainY: ndarray, labels of training data, the shape of it is (number of samples, )</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        labels = set(trainY.tolist())</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> labels:</span><br><span class="line">            x = trainX[trainY == y, :]</span><br><span class="line">            self.probability_of_y[y] = x.shape[<span class="number">0</span>] / trainX.shape[<span class="number">0</span>]</span><br><span class="line">            self.mean[y] = x.mean(axis = <span class="number">0</span>)</span><br><span class="line">            var = x.var(axis = <span class="number">0</span>)</span><br><span class="line">            var[var == <span class="number">0</span>] += <span class="number">1e-9</span> * var.max()</span><br><span class="line">            self.var[y] = var</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, testX)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        predict the labels of testX</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">            testX: ndarray, testing data, the shape of it is (number of samples, number of features)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">            ndarray: each element is a str variable, which represent the label of corresponding testing data</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        results = np.empty((testX.shape[<span class="number">0</span>], len(self.probability_of_y)))</span><br><span class="line">        labels = []</span><br><span class="line">        <span class="keyword">for</span> index, (label, py) <span class="keyword">in</span> enumerate(self.probability_of_y.items()):</span><br><span class="line">            t = np.exp(- ((testX - self.mean[label]) ** <span class="number">2</span>) / (<span class="number">2</span> * self.var[label])) / np.sqrt(<span class="number">2</span> * np.pi * self.var[label])</span><br><span class="line">            t[t == <span class="number">0</span>] = np.finfo(np.longdouble).eps</span><br><span class="line">            a = np.log(t)</span><br><span class="line">            results[:, index] = np.exp(np.sum(a, axis = <span class="number">1</span>)) * py</span><br><span class="line">            labels.append(label)</span><br><span class="line">        <span class="keyword">return</span> np.array(labels)[np.argmax(results, axis = <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(prediction, testY)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    compute accuracy for prediction</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">        prediction: ndarray, the prediction generated by the classifier</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        testY: ndarray, true labels</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">        float, accuracy</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> np.sum((prediction - testY) == <span class="number">0</span>) / testY.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    datadir = <span class="string">'SMSSpamCollection'</span></span><br><span class="line">    train_text, train_labels, test_text, test_labels = readDataSet(datadir)</span><br><span class="line">    trainX, trainY, words_table, labels_table = preprocessing_training_data(train_text, train_labels)</span><br><span class="line">    print(<span class="string">'training data shape:'</span>, trainX.shape, trainY.shape)</span><br><span class="line">    testX, testY = preprocessing_testing_data(test_text, test_labels, words_table, labels_table)</span><br><span class="line">    print(<span class="string">'testing data shape:'</span>, testX.shape, testY.shape)</span><br><span class="line">    a = GaussianNB()</span><br><span class="line">    a.fit(trainX, trainY)</span><br><span class="line">    r = a.predict(testX)</span><br><span class="line">    print(<span class="string">'accuracy:'</span>, accuracy(r, testY))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>按照上面的代码实现完后，和scikit-learn的Gaussian Naive Bayes做对比，发现精度查了20%左右……</p>
<p>然后就看了scikit-learn的实现，发现sklearn的实现将公式化简后实现的。</p>
<p>首先，我们的目标是：</p>
<script type="math/tex; mode=display">
y = \mathop{\arg\max}_{c_k} P(Y = c_k) \prod_j P(X^{(j)} = x^{(j)} \mid Y = c_k)</script><p>因为求这个的时候可能会出现下溢的情况，也就是很多项都很小的时候，这个连乘就会出问题。</p>
<p>为了解决这个问题，我原来以为是先取对数，再取指数，这样值就不变了，但是值用取对数就行，因为取对数不改变单调性。</p>
<p>那目标就变成了：</p>
<script type="math/tex; mode=display">
\begin{aligned}
y &= \mathop{\arg\max}_{c_k} \log^{P(Y = c_k) \prod_j P(X^{(j)} = x^{(j)} \mid Y = c_k)}\\
&= \mathop{\arg\max}_{c_k} [ \log^{P(y = c_k)} + \sum_j \log^{P(X^{(j)} = x^{(j)} \mid Y = c_k)} ]
\end{aligned}</script><p>在求条件概率的时候，也进行变换：</p>
<script type="math/tex; mode=display">\begin{aligned}
\log^{P(X^{(j)} = x^{(j)} \mid Y = c_k)} &= \log^{ \ \bigg[\frac{1}{\sqrt{2 \pi \sigma^2_{c_k,j}}} \exp{\bigg(- \frac{(a_{jl} - \mu_{c_k,j})^2}{2 \sigma^2_{c_k,j}}\bigg)}\bigg]}\\
&= \log^{ \frac{1}{\sqrt{2 \pi \sigma^2_{c_k,j}}} } + \log^{ \exp{\bigg(- \frac{(a_{jl} - \mu_{c_k,j})^2}{2 \sigma^2_{c_k,j}}\bigg)} }\\
&= - \frac{1}{2} \log^{2 \pi \sigma^2_{c_k,j}} - \frac{1}{2} \frac{(a_{jl} - \mu_{c_k,j})^2}{\sigma^2_{c_k,j}}
\end{aligned}</script><p>scikit-learn是实现的这个公式，我想了一下，这个公式比之前的那个公式要简洁很多，之前的公式中，如果求$e$的指数很小，就会出现0，如果手动补一个eps，又会影响结果。而且我们要对方差加eps，之前的公式中如果在方差上加了eps，求完指数运算后可能还要加eps，误差会逐渐的递增。</p>
<p>改进后的模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">myGaussianNB</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Gaussian naive bayes for continous features</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.label_mapping = dict()</span><br><span class="line">        self.probability_of_y = &#123;&#125;</span><br><span class="line">        self.mean = &#123;&#125;</span><br><span class="line">        self.var = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_clear</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.label_mapping.clear()</span><br><span class="line">        self.probability_of_y.clear()</span><br><span class="line">        self.mean.clear()</span><br><span class="line">        self.var.clear()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, trainX, trainY)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        use trainX and trainY to compute the prior probability for each class</span></span><br><span class="line"><span class="string">        and then compute the mean and variance for each features for each class</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">            trainX: ndarray, training data, the shape of it is (number of samples, number of features)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">            trainY: ndarray, labels of training data, the shape of it is (number of samples, )</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self._clear()</span><br><span class="line">        labels = np.unique(trainY)</span><br><span class="line">        self.label_mapping = &#123;label: index <span class="keyword">for</span> index, label <span class="keyword">in</span> enumerate(labels)&#125;</span><br><span class="line">        <span class="keyword">for</span> label <span class="keyword">in</span> labels:</span><br><span class="line">            x = trainX[trainY == label, :]</span><br><span class="line">            self.probability_of_y[label] = x.shape[<span class="number">0</span>] / trainX.shape[<span class="number">0</span>]</span><br><span class="line">            self.mean[label] = x.mean(axis = <span class="number">0</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">            self.var[label] = x.var(axis = <span class="number">0</span>, keepdims = <span class="literal">True</span>) + <span class="number">1e-9</span> * np.var(trainX, axis = <span class="number">0</span>).max()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, testX)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        predict the labels of testX</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">            testX: ndarray, testing data, the shape of it is (number of samples, number of features)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">            ndarray: each element is a str variable, which represent the label of corresponding testing data</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        results = np.empty((testX.shape[<span class="number">0</span>], len(self.probability_of_y)))</span><br><span class="line">        labels = [<span class="number">0</span>] * len(self.probability_of_y)</span><br><span class="line">        <span class="keyword">for</span> label, index <span class="keyword">in</span> self.label_mapping.items():</span><br><span class="line">            py = self.probability_of_y[label]</span><br><span class="line">            sum_of_conditional_probability = - <span class="number">0.5</span> * np.sum(((testX - self.mean[label]) ** <span class="number">2</span>) / self.var[label], <span class="number">1</span>)</span><br><span class="line">            sum_of_conditional_probability += - <span class="number">0.5</span> * np.sum(np.log(<span class="number">2</span> * np.pi * self.var[label]))</span><br><span class="line">            results[:, index] = sum_of_conditional_probability + np.log(py)</span><br><span class="line">            labels[index] = label</span><br><span class="line">        <span class="keyword">return</span> np.array(labels)[np.argmax(results, axis = <span class="number">1</span>)]</span><br></pre></td></tr></table></figure></div><div class="article-tags size-small is-uppercase mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/blog/tags/machine-learning/">machine learning</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/%E5%B7%B2%E5%A4%8D%E7%8E%B0/">已复现</a></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button is-info donate"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/blog/img/alipay.jpeg" alt="Alipay"></span></a><a class="button is-success donate"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/blog/img/wechat.jpeg" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/blog/2018/06/15/vscode-remote-workspace/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">vscode-remote-workspace</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/blog/2018/06/12/%E8%BF%81%E7%A7%BBcentos7%E8%99%9A%E6%8B%9F%E6%9C%BAmac%E5%92%8C%E7%BD%91%E5%8D%A1%E5%90%8D%E5%8F%98%E6%8D%A2%E5%AF%BC%E8%87%B4%E7%BD%91%E7%BB%9C%E4%B8%8D%E5%90%8C%E7%9A%84%E9%97%AE%E9%A2%98/"><span class="level-item">迁移CentOS7虚拟机mac和网卡名变换导致网络不通的问题</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="is-rounded" src="/blog/img/avatar.png" alt="Davidham"></figure><p class="title is-size-4 is-block line-height-inherit">Davidham</p><p class="is-size-6 is-block">阿里菜鸟-时空数据挖掘-算法工程师</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Hangzhou</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/blog/archives"><p class="title">87</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/blog/categories"><p class="title">8</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/blog/tags"><p class="title">38</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/davidham3" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/davidham3"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="mail" href="mailto:mengxian.sc@alibaba-inc.com"><i class="fa fa-envelope"></i></a></div></div></div><!--!--><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/blog/categories/algorithms/"><span class="level-start"><span class="level-item">algorithms</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/dataset/"><span class="level-start"><span class="level-item">dataset</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/software/"><span class="level-start"><span class="level-item">software</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B3%E5%8F%B0/"><span class="level-start"><span class="level-item">分布式平台</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">机器学习</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"><span class="level-start"><span class="level-item">自然语言处理</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">论文阅读笔记</span></span><span class="level-end"><span class="level-item tag">52</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/categories/%E9%9A%8F%E7%AC%94/"><span class="level-start"><span class="level-item">随笔</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/blog/tags/attention/"><span class="tag">Attention</span><span class="tag is-grey-lightest">5</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/graph/"><span class="tag">Graph</span><span class="tag is-grey-lightest">27</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/hadoop/"><span class="tag">Hadoop</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/kafka/"><span class="tag">Kafka</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/ner/"><span class="tag">NER</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/resnet/"><span class="tag">ResNet</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/sequence/"><span class="tag">Sequence</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/spark/"><span class="tag">Spark</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/spatial-temporal/"><span class="tag">Spatial-temporal</span><span class="tag is-grey-lightest">15</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/time-series/"><span class="tag">Time Series</span><span class="tag is-grey-lightest">12</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/algorithms/"><span class="tag">algorithms</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/computer-vision/"><span class="tag">computer vision</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/dataset/"><span class="tag">dataset</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/deep-learning/"><span class="tag">deep learning</span><span class="tag is-grey-lightest">50</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/event-sequence/"><span class="tag">event sequence</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/graph-convolutional-network/"><span class="tag">graph convolutional network</span><span class="tag is-grey-lightest">20</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/image-style-transfer/"><span class="tag">image style transfer</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/implicit-feedback/"><span class="tag">implicit feedback</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/language-modeling/"><span class="tag">language modeling</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/large-scale-learning/"><span class="tag">large-scale learning</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/learning-representations/"><span class="tag">learning representations</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/machine-learning/"><span class="tag">machine learning</span><span class="tag is-grey-lightest">19</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/machine-translation/"><span class="tag">machine translation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/multimodal/"><span class="tag">multimodal</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/natural-language-processing/"><span class="tag">natural language processing</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/normalization/"><span class="tag">normalization</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/point-process/"><span class="tag">point process</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/recommender-system/"><span class="tag">recommender system</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/reinforcement-learning/"><span class="tag">reinforcement learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/seq2seq/"><span class="tag">seq2seq</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/software/"><span class="tag">software</span><span class="tag is-grey-lightest">16</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/softwares/"><span class="tag">softwares</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/super-resolution/"><span class="tag">super resolution</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/time-series/"><span class="tag">time series</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/virtual-machine/"><span class="tag">virtual machine</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/vscode/"><span class="tag">vscode</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E5%B7%B2%E5%A4%8D%E7%8E%B0/"><span class="tag">已复现</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/%E9%9A%8F%E7%AC%94/"><span class="tag">随笔</span><span class="tag is-grey-lightest">1</span></a></div></div></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><h3 class="menu-label">Recent</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2022-05-24T15:10:01.000Z">2022-05-24</time></p><p class="title is-6"><a class="link-muted" href="/blog/2022/05/24/multibench-multiscale-benchmarks-for-multimodal-representation-learning/">MULTIBENCH: Multiscale Benchmarks for Multimodal Representation Learning</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2022-05-23T15:27:10.000Z">2022-05-23</time></p><p class="title is-6"><a class="link-muted" href="/blog/2022/05/23/semi-supervised-learning-for-marked-temporal-point-processes/">Semi-supervised Learning for Marked Temporal Point Processes</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2022-05-20T15:35:04.000Z">2022-05-20</time></p><p class="title is-6"><a class="link-muted" href="/blog/2022/05/20/individual-mobility-prediction-via-attentive-marked-temporal-point-processes/">Individual Mobility Prediction via Attentive Marked Temporal Point Processes</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2022-05-19T14:54:43.000Z">2022-05-19</time></p><p class="title is-6"><a class="link-muted" href="/blog/2022/05/19/unsupervised-scalable-representation-learning-for-multivariate-time-series/">Unsupervised Scalable Representation Learning for Multivariate Time Series</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2022-05-17T13:11:14.000Z">2022-05-17</time></p><p class="title is-6"><a class="link-muted" href="/blog/2022/05/17/fully-neural-network-based-model-for-general-temporal-point-processes/">Fully Neural Network based Model for General Temporal Point Processes</a></p><p class="is-uppercase"><a class="link-muted" href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/blog/archives/2022/05/"><span class="level-start"><span class="level-item">May 2022</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2020/05/"><span class="level-start"><span class="level-item">May 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2020/02/"><span class="level-start"><span class="level-item">February 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2020/01/"><span class="level-start"><span class="level-item">January 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/12/"><span class="level-start"><span class="level-item">December 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/11/"><span class="level-start"><span class="level-item">November 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/08/"><span class="level-start"><span class="level-item">August 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/07/"><span class="level-start"><span class="level-item">July 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/06/"><span class="level-start"><span class="level-item">June 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/05/"><span class="level-start"><span class="level-item">May 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/04/"><span class="level-start"><span class="level-item">April 2019</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/03/"><span class="level-start"><span class="level-item">March 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/02/"><span class="level-start"><span class="level-item">February 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2019/01/"><span class="level-start"><span class="level-item">January 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/12/"><span class="level-start"><span class="level-item">December 2018</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/10/"><span class="level-start"><span class="level-item">October 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/09/"><span class="level-start"><span class="level-item">September 2018</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/08/"><span class="level-start"><span class="level-item">August 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/07/"><span class="level-start"><span class="level-item">July 2018</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/06/"><span class="level-start"><span class="level-item">June 2018</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/05/"><span class="level-start"><span class="level-item">May 2018</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/04/"><span class="level-start"><span class="level-item">April 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/03/"><span class="level-start"><span class="level-item">March 2018</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/02/"><span class="level-start"><span class="level-item">February 2018</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2018/01/"><span class="level-start"><span class="level-item">January 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/blog/archives/2017/08/"><span class="level-start"><span class="level-item">August 2017</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/"><img src="/blog/img/logo.svg" alt="Davidham&#039;s blog" height="28"></a><p class="size-small"><span>&copy; 2022 Davidham</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://davidham3.github.io',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/blog/js/back_to_top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>